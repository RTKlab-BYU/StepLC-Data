{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from scipy.stats import f_oneway as anova\n",
    "import scipy\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import plot\n",
    "import json\n",
    "import plotly.io as pio\n",
    "from plotly.offline import iplot\n",
    "import plotly as py\n",
    "from matplotlib_venn import venn2, venn3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import plotly.colors\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import t\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.formula.api import ols \n",
    "  \n",
    "import math\n",
    "from select import select\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "#only used for the app\n",
    "\n",
    "# import django\n",
    "# from django.conf import settings\n",
    "# from django.contrib.auth.decorators import login_required, permission_required\n",
    "# from file_manager.models import DataAnalysisQueue, SampleRecord, \\\n",
    "#     SavedVisualization, VisualizationApp, UserSettings, ProcessingApp\n",
    "# from django.shortcuts import render\n",
    "# from django.conf import settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants \n",
    "saved_settings ={}\n",
    "plot_options = {}\n",
    "JUPYTER_MODE = \"JPY_PARENT_PID\" in os.environ #check if it's in jupiter notebook mode\n",
    "APPFOLDER = \"./\"\n",
    "url_base = None\n",
    "\n",
    "#settings\n",
    "WRITE_OUTPUT = True\n",
    "USE_MaxLFQ = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is only for the webapp, not for jupyter notebook\n",
    "def get_run_name(queue_id):\n",
    "    \"\"\"_Get the run name/sample list from the result files, only\n",
    "    used for webapp for populate dropdown list_\n",
    "    Args:\n",
    "        queue_id (_int_): _task id from the process queue_\n",
    "    Returns:\n",
    "        _type_: _pandas data serial contains experiment list_\n",
    "        0              sample1\n",
    "        1              sample2\n",
    "        2              sample3\n",
    "    \"\"\"\n",
    "    if not queue_id:\n",
    "        return None\n",
    "    # get processing name\n",
    "    process_app = DataAnalysisQueue.objects.filter(\n",
    "        pk=queue_id).first().processing_app.name\n",
    "    # fragpipe results\n",
    "    if \"FragPipe\" in process_app:\n",
    "        peptide_file = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2\n",
    "        peptide = pd.read_table(peptide_file)\n",
    "        #\n",
    "        # get experiment names from columns names containning \" MaxLFQ Intensity\"\n",
    "        run_metadata = [\n",
    "            col for col in peptide.columns if \" MaxLFQ Intensity\" in col]\n",
    "        # remove \" MaxLFQ Intensity\" from the experiment names\n",
    "        run_metadata = [name.replace(\" MaxLFQ Intensity\", \"\")\n",
    "                            for name in run_metadata]\n",
    "        # create a pandas series to store the experiment names\n",
    "        run_metadata = pd.Series(run_metadata)\n",
    "    elif \"PD\" in process_app:\n",
    "        inpufile_6 = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_6\n",
    "        meta_table = pd.read_table(inpufile_6)\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        run_metadata =meta_table['file_names']\n",
    "\n",
    "    else:\n",
    "        run_metadata = pd.Series()\n",
    "\n",
    "    return run_metadata\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf24402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only for the webapp, not for jupyter notebook\n",
    "\n",
    "# def queue_info_api(queue_id, server_address, user_name, password):\n",
    "#     \"\"\"_Get the queue and app info from the server through API, this\n",
    "#     is only for the jupyter notebook, not for the webapp_\n",
    "#     \"\"\"\n",
    "\n",
    "#     authinfo = HTTPBasicAuth(user_name, password)\n",
    "\n",
    "#     #get queue info and test if the user name and password are correct   \n",
    "\n",
    "#     queue_response = requests.get(\n",
    "#         f'http://{server_address}/files/api/DataAnalysisQueue/{queue_id}/',\n",
    "#         auth=authinfo\n",
    "#     )\n",
    "#     if queue_response.status_code != 200:\n",
    "#         raise Exception(\"Invalid username or password\")\n",
    "#     else:\n",
    "#         queue_json_data = queue_response.json()\n",
    "\n",
    "#     # Get app information\n",
    "#     app_response = requests.get(\n",
    "#         f\"http://{server_address}/files/api/ProcessingApp/{queue_json_data['processing_app']}/\",\n",
    "#         auth=authinfo\n",
    "#     )\n",
    "#     app_json_data = app_response.json()\n",
    "#     return queue_json_data, app_json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abd47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumIDs(IDMatrix):\n",
    "    \"\"\"_summarize the ID matrix infor into ID summary_\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        IDMatrix (_type_): _protein or pepetides matrix_\n",
    "        0 Accession/Annotated Sequence \trun1 \trun2 \trun3 \n",
    "        1 P023D12\tMS2 \tMBR \tNaN \n",
    "        2 P1222\tNaN \tID \tNaN \n",
    "    ID: means we don't know the ID mode\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "                                      names  MS2_IDs  MBR_IDs  Total_IDs\n",
    "0            10ng_QC_1_channel2 Intensity      NaN      NaN       3650\n",
    "1            10ng_QC_2_channel1 Intensity      NaN      NaN       3604\n",
    "....\n",
    "    \"\"\"\n",
    "    # removes the columns that don't have ID data\n",
    "    columns = [col for col in IDMatrix.columns if not any(\n",
    "        substring in col for substring in [\n",
    "            'Accession', 'Annotated Sequence'])]\n",
    "    #put each ID_Modes into a list\n",
    "    returnNames = []\n",
    "    MS2_ID = []\n",
    "    MBR_ID = []\n",
    "    total_ID = []\n",
    "    for eachColumn in columns:\n",
    "        MS2_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MS2\"])) #PD differentiates\n",
    "        MBR_ID.append(len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"MBR\"])) #PD differentiates\n",
    "        total_ID_each = len(IDMatrix[eachColumn][IDMatrix[eachColumn] == \"ID\"]) #some don't so we count total directly\n",
    "        if total_ID_each == 0: #otherwise we sum\n",
    "            total_ID_each = len(IDMatrix[eachColumn][\n",
    "                IDMatrix[eachColumn] == \"MS2\"]) + len(IDMatrix[\n",
    "                eachColumn][IDMatrix[eachColumn] == \"MBR\"])\n",
    "        total_ID.append(total_ID_each)\n",
    "\n",
    "    return pd.DataFrame({'names': columns,\n",
    "                         'MS2_IDs': MS2_ID,\n",
    "                         'MBR_IDs': MBR_ID,\n",
    "                         'Total_IDs': total_ID})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c7820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_from_name_mapping(columns, partial_column_name_mapping):\n",
    "    #input is column names, and a dictionary with what you want each column (key) to be renamed to (value)\n",
    "    column_name_mapping = {}\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key in col: #in the case of PD, we are looking for a pattern within the column name\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_to_name_mapping(columns, partial_column_name_mapping):\n",
    "    #input is column names, and a dictionary with what you want each column (key) to be renamed to (value)\n",
    "    column_name_mapping = {}\n",
    "    for col in columns:\n",
    "        for key, value in partial_column_name_mapping.items():\n",
    "            if key == col:  #after we get away from PD's weirdness, then we want exact matches,\n",
    "                            #so we don't get for example 1-11 when we look for 1-1 for file Identifiers or filenames\n",
    "                column_name_mapping[col] = value\n",
    "                break\n",
    "    return column_name_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b07534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_IDs(all_matrix, MS2_matrix):\n",
    "    # make IDs into MBR\n",
    "    if \"Annotated Sequence\" in all_matrix.columns:\n",
    "        name = \"Annotated Sequence\"\n",
    "    elif \"Accession\" in all_matrix.columns:  \n",
    "        name = \"Accession\"\n",
    "    id_cols = all_matrix.columns.tolist()\n",
    "    id_cols.remove(name)\n",
    "    \n",
    "    \n",
    "    # for eachColumn in id_cols:\n",
    "    #     if len(all_matrix[(all_matrix[name].isin(MS2_matrix[name]) & (MS2_matrix[eachColumn] == \"MS2\"))]) > 0:\n",
    "    #         all_matrix.loc[(all_matrix[name].isin(MS2_matrix[name]) & (MS2_matrix[eachColumn] == \"MS2\")), [eachColumn]] = MS2_matrix[[eachColumn]]\n",
    "\n",
    "    all_keys = pd.merge(all_matrix[name], MS2_matrix[name],how=\"outer\")\n",
    "    # print(all_keys)\n",
    "\n",
    "    all_matrix = pd.merge(all_matrix, all_keys, how=\"right\").replace(\"ID\",\"MBR\")\n",
    "    MS2_matrix = pd.merge(MS2_matrix, all_keys, how=\"right\")\n",
    "\n",
    "    # print(all_matrix)\n",
    "    for eachColumn in id_cols:\n",
    "        # print(len(all_matrix[eachColumn]))\n",
    "        # print(len(MS2_matrix[eachColumn]))\n",
    "        if len(all_matrix[(all_matrix[name].isin(MS2_matrix[name]) & (MS2_matrix[eachColumn] == \"MS2\"))]) > 0:\n",
    "            all_matrix.loc[(all_matrix[name].isin(MS2_matrix[name]) & (MS2_matrix[eachColumn] == \"MS2\")), [eachColumn]] = \"MS2\"\n",
    "\n",
    "    # print(all_matrix)\n",
    "\n",
    "    return all_matrix #noticed this changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(queue_id=None, queue_info= None, processor_info = None,\n",
    "               input1=None, input2=None,input3=None, input4=None, input5=None,\n",
    "            process_app = None, file_id = 1):\n",
    "    \"\"\"_Read data from data manager API or through local files or read directly\n",
    "    in the webapp_\n",
    "    Args:\n",
    "        queue_id (_int_): _processing queue id_\n",
    "        queue_info (_dict_): _queue info from the API_\n",
    "        processor_info (_dict_): _processor info from the API_        \n",
    "        input1 (_str_): _input file 1_ \n",
    "        input2 (_str_): _input file 2_\n",
    "        input3 (_str_): _input file 3_\n",
    "        input4 (_str_): _input file 4_\n",
    "        input5 (_str_): _input file 5_\n",
    "        process_app (_str_): _process app name_\n",
    "    Returns:\n",
    "        _dict_: _dictionary containing data all data        \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" Input files are as followws\n",
    "    App       Input file\n",
    "              1                         2                       3         4         5\n",
    "    PD        _Proteins                 _PeptideGroups                              _InputFiles\n",
    "    Fragpipe  combined_protein          combined_peptide\n",
    "    DIANN     diann-output.pg_matrix    diann-output.pr_matrix  protein   peptide   filelist_diann.txt\n",
    "    \"\"\"\n",
    "\n",
    "    min_unique_peptides = 1\n",
    "\n",
    "    #getting files from data system\n",
    "        # getting files from data system (webapp)\n",
    "    if queue_id is not None and processor_info is None:\n",
    "        # Method 1 pull data directly (used by the webapp)\n",
    "        process_app = DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().processing_app.name\n",
    "        input1= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_1\n",
    "        input2= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_2  \n",
    "        input3= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_3\n",
    "        input4= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_4  \n",
    "        input5= DataAnalysisQueue.objects.filter(\n",
    "            pk=queue_id).first().output_file_5\n",
    "    elif queue_info is not None and processor_info is not None:\n",
    "    # Method 2 pull data from the data system (used by jupyter notebook)\n",
    "        process_app = processor_info[\"name\"]\n",
    "        input1= queue_info[\"output_file_1\"]\n",
    "        input2= queue_info[\"output_file_2\"]  \n",
    "        input3= queue_info[\"output_file_3\"]\n",
    "        input4= queue_info[\"output_file_4\"]  \n",
    "        input5= queue_info[\"output_file_5\"]\n",
    "    # method 3 feed data directly (through local file paths)\n",
    "    else:\n",
    "        analysis_file = input1\n",
    "\n",
    "    if \"FragPipe\" in process_app:     # fragpipe results\n",
    "        # read data\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "\n",
    "        # filter Contaminant\n",
    "        peptide_table= peptide_table[~peptide_table[\n",
    "            'Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        protein_table= protein_table[~protein_table['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)].query(\n",
    "            \"`Combined Total Peptides` >= @min_unique_peptides\")\n",
    "        # get experiment names from columns names containning \"Intensity\"\n",
    "        # or \" MaxLFQ Intensity\" if MaxLFQ is used\n",
    "\n",
    "        if USE_MaxLFQ:\n",
    "            column_tail = \" MaxLFQ Intensity\"\n",
    "            intensity_cols = protein_table.columns[\n",
    "                protein_table.columns.str.contains(column_tail)].tolist() #These should be the same for MS2\n",
    "        else:\n",
    "            column_tail = \" Intensity\" #this includes MaxLFQ Intensity\n",
    "            cols = protein_table.columns\n",
    "            intensity_cols =  protein_table.columns[( \\\n",
    "                cols.str.contains(\" Intensity\")) & \\\n",
    "                (~cols.str.contains(\" MaxLFQ Intensity\"))]\n",
    "        prot_ID_cols = protein_table.columns[(cols.str.contains(\" Unique Spectral Count\")) & ~(cols == \"Combined Unique Spectral Count\")] #so we remove MaxLFQ Intensity here\n",
    "        cols = peptide_table.columns\n",
    "        pep_ID_cols = peptide_table.columns[(cols.str.contains(\" Spectral Count\"))] #so we remove MaxLFQ Intensity here\n",
    "        #get the column names of protein_table that contain \"Intensity\" but not \" MaxLFQ\"\n",
    "        \n",
    "        # ALL\n",
    "        ## Proteins abundance table\n",
    "        protein_table.rename(columns={'Protein ID': 'Accession'},inplace=True)\n",
    "        all_abundance_cols = intensity_cols.append(pd.Index(['Accession']))\n",
    "        prot_abundance = protein_table.loc[:, all_abundance_cols]\n",
    "        ## Peptide abundance table\n",
    "        peptide_table.rename(columns={'Peptide Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "        all_abundance_cols = intensity_cols.append(pd.Index(['Annotated Sequence']))\n",
    "        pep_abundance = peptide_table.loc[:, all_abundance_cols]\n",
    "\n",
    "        # Proteins ID table\n",
    "        all_ID_cols = prot_ID_cols.append(pd.Index(['Accession']))\n",
    "        prot_ID_MS2 = protein_table.loc[:, all_ID_cols]\n",
    "        \n",
    "        # Proteins ID table\n",
    "        all_ID_cols = pep_ID_cols.append(pd.Index(['Annotated Sequence']))\n",
    "        pep_ID_MS2 = peptide_table.loc[:, all_ID_cols]\n",
    "\n",
    "\n",
    "        # remove \"Spectral Count\", \" MaxLFQ Intensity\" or \" Intensity\" from names\n",
    "        run_name_list = [name.replace(column_tail, \"\")\n",
    "                            for name in intensity_cols]\n",
    "        \n",
    "        run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "        prot_abundance = prot_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              prot_abundance.columns if column_tail in col})\n",
    "\n",
    "        pep_abundance = pep_abundance.rename(columns={\n",
    "            col: col.replace(column_tail, \"\") for col in\n",
    "              pep_abundance.columns if column_tail in col})\n",
    "        \n",
    "        prot_ID_MS2 = prot_ID_MS2.rename(columns={\n",
    "            col: col.replace(\" Unique Spectral Count\", \"\") for col in\n",
    "              prot_ID_MS2.columns if \" Unique Spectral Count\" in col})\n",
    "\n",
    "        pep_ID_MS2 = pep_ID_MS2.rename(columns={\n",
    "            col: col.replace(\" Spectral Count\", \"\") for col in\n",
    "              pep_ID_MS2.columns if \" Spectral Count\" in col})\n",
    "\n",
    "        # print(pep_ID_MS2.columns)\n",
    "\n",
    "        for item in [prot_abundance,pep_abundance,pep_ID_MS2,prot_ID_MS2]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        # get ID matrix tables\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Annotated Sequence\t']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        #Rename protein numbers\n",
    "        \n",
    "        cols = [col for col in prot_ID_MS2.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID_MS2[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID_MS2[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID_MS2[col] = prot_ID_MS2[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"MS2\", regex=True)\n",
    "        cols = [col for col in pep_ID_MS2.columns if col != 'Annotated Sequence\t']\n",
    "        for col in cols:\n",
    "            if pep_ID_MS2[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID_MS2[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID_MS2[col] = pep_ID_MS2[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"MS2\", regex=True)\n",
    "\n",
    "        # print(run_name_list)\n",
    "        pep_ID = combine_IDs(pep_ID, pep_ID_MS2)\n",
    "        prot_ID = combine_IDs(prot_ID, prot_ID_MS2)\n",
    "\n",
    "        prot_other_info = protein_table.loc[\n",
    "            :, ~protein_table.columns.str.contains('Intensity')]\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info = peptide_table.loc[\n",
    "            :, ~peptide_table.columns.str.contains('Intensity')]\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "\n",
    "    elif \"DIANN\" in process_app:\n",
    "        # read in DIANN output files\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        protein_table_MS2 = pd.read_table(input3,low_memory=False)\n",
    "        peptide_table_MS2 = pd.read_table(input4,low_memory=False)\n",
    "        prot_other_info = pd.DataFrame({\"Protein\": protein_table[\"Protein.Ids\"], \"Protein.Group\": protein_table[\"Protein.Group\"]})\n",
    "        pep_other_info = pd.DataFrame({\"Mapped Proteins\": peptide_table[\"Protein.Group\"], \"Modified.Sequence\": peptide_table[\"Modified.Sequence\"]})\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = \"None\"\n",
    "        pep_other_info[\"Source_File\"] = \"None\"\n",
    "\n",
    "        # meta_table = pd.read_csv(input5, sep=' ', header=None, names=[\"File Name\"])\n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[~protein_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        peptide_table= peptide_table[~peptide_table['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        protein_table_MS2= protein_table_MS2[~protein_table_MS2['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        peptide_table_MS2= peptide_table_MS2[~peptide_table_MS2['Protein.Group'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        prot_other_info= prot_other_info[~prot_other_info['Protein'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        pep_other_info= pep_other_info[~pep_other_info['Mapped Proteins'].str.contains(\n",
    "            \"contam_sp\", na=False)]\n",
    "        \n",
    "        prot_other_info.rename(columns={'Protein': 'Accession'}, inplace=True)\n",
    "        pep_other_info.rename(columns={'Modified.Sequence': 'Annotated Sequence'}, inplace=True)\n",
    "        # Replace backslashes with forward slashes if data comes from Windows\n",
    "        # meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # # Apply a lambda function to extract file names without extensions\n",
    "        # meta_table['File Name'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        # run_name_list = meta_table['File Name'].tolist()\n",
    "        # run_name_list = pd.DataFrame({\"Run Names\": run_name_list})\n",
    "        # run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "\n",
    "\n",
    "        # Get the file names from the meta table\n",
    "        protein_path_cols = protein_table_MS2.filter(regex='\\\\\\\\|Protein.Ids').columns\n",
    "\n",
    "        ## Proteins\n",
    "        prot_abundance = protein_table.loc[:, protein_path_cols]\n",
    "        prot_abundance_MS2 = protein_table_MS2.loc[:, protein_path_cols]\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = protein_table.filter(regex='\\\\\\\\').columns\n",
    "        prot_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in prot_abundance.columns]\n",
    "        prot_abundance = prot_abundance.rename(columns={'Protein.Ids': 'Accession'})\n",
    "        prot_abundance_MS2.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in prot_abundance_MS2.columns]\n",
    "        prot_abundance_MS2 = prot_abundance_MS2.rename(columns={'Protein.Ids': 'Accession'})   \n",
    "\n",
    "        ## Peptides\n",
    "        peptide_path_cols = peptide_table_MS2.filter(regex='\\\\\\\\|Modified.Sequence').columns\n",
    "        pep_abundance = peptide_table.loc[:, peptide_path_cols]\n",
    "        pep_abundance_MS2 = peptide_table_MS2.loc[:, peptide_path_cols]\n",
    "        # Rename Columns to remove file path\n",
    "        file_path_cols = peptide_table.filter(regex='\\\\\\\\').columns\n",
    "        pep_abundance.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in pep_abundance.columns]\n",
    "        pep_abundance = pep_abundance.rename(columns={'Modified.Sequence': 'Annotated Sequence'})\n",
    "        pep_abundance_MS2.columns = [os.path.splitext(os.path.basename(x))[0] if x in file_path_cols else x for x in pep_abundance_MS2.columns]\n",
    "        pep_abundance_MS2 = pep_abundance_MS2.rename(columns={'Modified.Sequence': 'Annotated Sequence'})\n",
    "\n",
    "        run_name_list = pd.DataFrame(data={\"Run Names\": file_path_cols})\n",
    "\n",
    "        for item in [prot_abundance,pep_abundance,prot_abundance_MS2,pep_abundance_MS2]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "\n",
    "\n",
    "        #convert to str for IDs matrix\n",
    "        pep_ID = pep_abundance.copy()\n",
    "        cols = [col for col in pep_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if pep_ID[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID[col] = pep_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        pep_ID_MS2 = pep_abundance_MS2.copy()\n",
    "        cols = [col for col in pep_ID_MS2.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if pep_ID_MS2[col].dtype != 'object': # Check if not a string column\n",
    "                pep_ID_MS2[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                pep_ID_MS2[col] = pep_ID_MS2[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"MS2\", regex=True)\n",
    "        prot_ID = prot_abundance.copy()\n",
    "        cols = [col for col in prot_ID.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID[col] = prot_ID[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"ID\", regex=True)\n",
    "        prot_ID_MS2 = prot_abundance_MS2.copy()\n",
    "        cols = [col for col in prot_ID_MS2.columns if col != 'Accession']\n",
    "        for col in cols:\n",
    "            if prot_ID_MS2[col].dtype != 'object': # Check if not a string column\n",
    "                prot_ID_MS2[col].replace(0, np.nan, inplace=True)\n",
    "                # Replace all numerical values to ID\n",
    "                prot_ID_MS2[col] = prot_ID_MS2[col].astype(str).str.replace(\"\\d+\\.\\d+\", \"MS2\", regex=True)\n",
    "\n",
    "        pep_ID = combine_IDs(pep_ID, pep_ID_MS2)\n",
    "        prot_ID = combine_IDs(prot_ID, prot_ID_MS2)       \n",
    "        \n",
    "    elif \"PD\" in process_app:\n",
    "        peptide_table = pd.read_table(input2,low_memory=False)\n",
    "        protein_table = pd.read_table(input1,low_memory=False)\n",
    "        \n",
    "        # filter Contaminant\n",
    "        protein_table= protein_table[(protein_table[\n",
    "            \"Protein FDR Confidence: Combined\"] == \"High\") &\n",
    "                        ((protein_table[\"Master\"] == \"IsMasterProtein\") | \n",
    "                         (protein_table[\"Master\"] == \"Master\")) & \n",
    "                        (protein_table[\"Contaminant\"] == False)]\n",
    "\n",
    "        protein_table.rename(\n",
    "            columns={'# Peptides': 'number of peptides'}, inplace=True)\n",
    "        protein_table=protein_table.query(\n",
    "            \"`number of peptides` >= @min_unique_peptides\")\n",
    "        peptide_table= peptide_table[(peptide_table[\n",
    "            'Contaminant'] == False) & (peptide_table[\"Confidence\"]== \"High\")]\n",
    "\n",
    "        meta_table = pd.read_table(input5,low_memory=False)\n",
    "        #filter rows in meta table on File ID column if it is NaN\n",
    "        meta_table = meta_table[meta_table['File ID'].notna()]\n",
    "\n",
    "        # Replace single backslashes with forward slashes in the 'file_paths' column\n",
    "        meta_table['File Name'] = meta_table['File Name'].str.replace('\\\\', '/', regex=False)\n",
    "        # Apply a lambda function to extract file names without extensions\n",
    "        meta_table['file_names'] = meta_table['File Name'].apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n",
    "        file_path_name_dict = dict(zip(meta_table['File ID'], meta_table['file_names']))\n",
    "        run_name_list = pd.DataFrame({\"Run Names\": file_path_name_dict.values()})\n",
    "        run_name_list['Run Identifier'] = run_name_list.index.to_series().apply(lambda x: str(file_id) + \"-\" + str(x))\n",
    "        \n",
    "        #format the read in table into three different tables: abundance, id and other_info\n",
    "        prot_abundance = protein_table.filter(regex='Abundance:|Accession')\n",
    "        prot_ID = protein_table.filter(regex='Found in Sample:|Accession')\n",
    "        prot_other_info = protein_table.loc[:, ~protein_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "        \n",
    "\n",
    "        pep_abundance = peptide_table.filter(regex='Abundance:|Annotated Sequence')\n",
    "        pep_ID = peptide_table.filter(regex='Found in Sample:|Annotated Sequence')\n",
    "        pep_other_info = peptide_table.loc[:, ~peptide_table.columns.str.contains('Found in Sample:|Abundance:')]\n",
    "\n",
    "        prot_other_info[\"Source_File\"] = input1\n",
    "        pep_other_info[\"Source_File\"] = input2\n",
    "\n",
    "        #change column names to file/run names to our fileID\n",
    "\n",
    "        new_dict = {\"Abundance: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [prot_abundance,pep_abundance]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "            #TODO solving  A value is trying to be set on a copy of a slice from a DataFrame\n",
    "            \n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            #use generate_column_to_name_mapping function because we don't want partial matches as in QC_HeLa.raw and QC_HeLa_20230727235101.raw\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "        \n",
    "\n",
    "        new_dict = {\"Found in Sample: \" + key + \":\": value for key, value in file_path_name_dict.items()}\n",
    "        for item in [pep_ID,prot_ID]:\n",
    "            # Generate a new column name mapping using the function\n",
    "            column_name_mapping = generate_column_from_name_mapping(item.columns, new_dict)\n",
    "\n",
    "            item.rename(columns = column_name_mapping, inplace = True)\n",
    "            #use generate_column_to_name_mapping function because we don't want partial matches\n",
    "            fileid_mapping = generate_column_to_name_mapping(item.columns, dict(zip(run_name_list[\"Run Names\"],run_name_list[\"Run Identifier\"])))\n",
    "\n",
    "            item.rename(columns = fileid_mapping,inplace=True)\n",
    "\n",
    "        # replace \"High\" to MS2 \"Peak Found\" to MBR, the rest become np.NaN\n",
    "        replacements = {'High': 'MS2', 'Peak Found': 'MBR', \"Medium\": np.NaN, \"Low\": np.NaN, \"Not Found\": np.NaN}\n",
    "        for column in run_name_list[\"Run Identifier\"]:\n",
    "            if column in pep_ID.columns:\n",
    "                pep_ID[column] = pep_ID[column].replace(to_replace=replacements)\n",
    "    \n",
    "            if column in prot_ID.columns:\n",
    "                prot_ID[column] = prot_ID[column].replace(to_replace=replacements)\n",
    "    \n",
    "    # get ID summary by parsing ID Matrix\n",
    "    protein_ID_summary = sumIDs(prot_ID)\n",
    "    peptide_ID_summary = sumIDs(pep_ID)\n",
    "    \n",
    "\n",
    "    #sets the processing app in run_name_list\n",
    "    run_name_list[\"Processing App\"] = process_app\n",
    "    run_name_list[\"Analysis Name\"] = analysis_file\n",
    "\n",
    "\n",
    "    return {'run_metadata': run_name_list,\n",
    "            'protein_other_info': prot_other_info,\n",
    "            'peptide_other_info': pep_other_info,\n",
    "            'protein_abundance': prot_abundance,\n",
    "            'protein_ID_matrix': prot_ID,\n",
    "            'protein_ID_Summary': protein_ID_summary,\n",
    "            'peptide_abundance': pep_abundance,\n",
    "            'peptide_ID_matrix': pep_ID,\n",
    "            'peptide_ID_Summary': peptide_ID_summary,\n",
    "\n",
    "            }  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(queue_ids = None, queue_info = None, processor_info = None, grouped_input_files = []):\n",
    "    '''\n",
    "    Creates a list of data objects\n",
    "    \n",
    "    Input\n",
    "    grouped_input_files\n",
    "    [\n",
    "        #File 0\n",
    "    {input1:\n",
    "    input2:\n",
    "    input3:   \n",
    "    input4:\n",
    "    input5:\n",
    "    process_app:\n",
    "    },\n",
    "        #File 1\n",
    "    {input1:\n",
    "    input2:\n",
    "    input3:   \n",
    "    input4:\n",
    "    input5:\n",
    "    process_app:\n",
    "    },\n",
    "    ...\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "\n",
    "    data_objects = []\n",
    "\n",
    "    i = 0\n",
    "    for eachGroup in grouped_input_files:\n",
    "        if queue_ids is not None:\n",
    "            pass\n",
    "        else:\n",
    "            process_app = eachGroup[\"process_app\"]\n",
    "            input1= eachGroup[\"input1\"]\n",
    "            input2= eachGroup[\"input2\"]  \n",
    "            input3= eachGroup[\"input3\"]\n",
    "            input4= eachGroup[\"input4\"]  \n",
    "            input5= eachGroup[\"input5\"]\n",
    "\n",
    "        current_data_object = read_file(input1=input1,input2=input2,\n",
    "                                        input3=input3,input4 = input4,\n",
    "                                        input5=input5, process_app=process_app,file_id = i)\n",
    "        data_objects.append(current_data_object)\n",
    "        \n",
    "        i = i + 1\n",
    "\n",
    "    \n",
    "    return data_objects\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b6b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_join_data_objects(data_objects):\n",
    "    '''\n",
    "    Takes in a list of data objects as given by read_files and converts them to a single data object as given by read_files,\n",
    "    protein info continues to show what was found on each original file, and so forth.\n",
    "    '''\n",
    "\n",
    "    first_file = True\n",
    "    for eachDataObject in data_objects:\n",
    "        print(\"***\")\n",
    "        if first_file:\n",
    "            first_file = False\n",
    "            final_data_object = eachDataObject\n",
    "        else:\n",
    "            final_data_object['run_metadata'] = pd.concat([final_data_object['run_metadata'],eachDataObject['run_metadata']]).reset_index(drop=True)\n",
    "            final_data_object['protein_other_info'] = pd.concat([final_data_object['protein_other_info'],eachDataObject['protein_other_info']]).reset_index(drop=True)\n",
    "            final_data_object['peptide_other_info'] = pd.concat([final_data_object['peptide_other_info'],eachDataObject['peptide_other_info']]).reset_index(drop=True)\n",
    "            final_data_object['protein_ID_Summary'] = pd.concat([final_data_object['protein_ID_Summary'],eachDataObject['protein_ID_Summary']]).reset_index(drop=True)\n",
    "            final_data_object['peptide_ID_Summary'] = pd.concat([final_data_object['peptide_ID_Summary'],eachDataObject['peptide_ID_Summary']]).reset_index(drop=True)\n",
    "            duplicates_found = False\n",
    "            \n",
    "            #loop through to see if there are any duplicate files\n",
    "            for eachCol in final_data_object['protein_abundance'].loc[:, final_data_object['protein_abundance'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['protein_ID_matrix'].loc[:, final_data_object['protein_ID_matrix'].columns!='Accession'].columns:\n",
    "                if eachCol in eachDataObject['protein_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_abundance'].loc[:, final_data_object['peptide_abundance'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_abundance'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass\n",
    "            for eachCol in final_data_object['peptide_ID_matrix'].loc[:, final_data_object['peptide_ID_matrix'].columns!='Annotated Sequence'].columns:\n",
    "                if eachCol in eachDataObject['peptide_ID_matrix'].columns:\n",
    "                    duplicates_found = True\n",
    "                else:\n",
    "                    pass     \n",
    "            if duplicates_found:\n",
    "                print(\"Error: files analyzed twice present!!!\")\n",
    "                quit()\n",
    "                print(\"@#afio2q3\")\n",
    "            else:\n",
    "                #merge keeping all proteins\n",
    "                # print(\"!!!!\")\n",
    "                final_data_object['protein_abundance'] = pd.merge(final_data_object['protein_abundance'],eachDataObject['protein_abundance'],how=\"outer\")\n",
    "                final_data_object['protein_ID_matrix'] = pd.merge(final_data_object['protein_ID_matrix'],eachDataObject['protein_ID_matrix'],how=\"outer\")\n",
    "                final_data_object['peptide_abundance'] = pd.merge(final_data_object['peptide_abundance'],eachDataObject['peptide_abundance'],how=\"outer\")\n",
    "                final_data_object['peptide_ID_matrix'] = pd.merge(final_data_object['peptide_ID_matrix'],eachDataObject['peptide_ID_matrix'],how=\"outer\")\n",
    "                \n",
    "    return final_data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_missing_values(data_object,\n",
    "                             missing_value_thresh=33,\n",
    "                             is_protein=True,\n",
    "                             ignore_nan=False):\n",
    "    \"\"\"_Filter out proteins/peptides with missing values rate above the\n",
    "    threshold_\n",
    "\n",
    "    Args:\n",
    "        data_object (_panada_): _dataframe contain data for one experimental\n",
    "        condition_\n",
    "        missing_value_thresh (int, optional): _description_. Defaults to 33.\n",
    "        analysis_program (str, optional): _description_.\n",
    "        ignore_nan: if filter intensity again with Nan threadshold, this \n",
    "        helps with the calcualting stdev step.\n",
    "\n",
    "    Returns:\n",
    "        _data_object_: _dictionary containing data for one experimental\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2 3_TrypsinLysConly_3BC_channel1\n",
    "0     A0A096LP49                            0.00                                        10\n",
    "1     A0A0B4J2D5                        89850.26                                      3311\n",
    "2         A0AVT1                        83055.87                                    312312\n",
    "    \"\"\"\n",
    "    if is_protein:\n",
    "        name = \"Accession\"\n",
    "        matrix_name = \"protein_ID_matrix\"\n",
    "        other_info_name = \"protein_other_info\"\n",
    "        abundance_name = \"protein_abundance\"\n",
    "        \n",
    "    else:\n",
    "        name = \"Annotated Sequence\"\n",
    "        matrix_name = \"peptide_ID_matrix\"\n",
    "        other_info_name = \"peptide_other_info\"\n",
    "        abundance_name = \"peptide_abundance\"\n",
    "\n",
    "    #initializes number of missing values to zero\n",
    "    protein_columns = data_object[matrix_name].assign(missingValues=0)\n",
    "\n",
    "    i = 0\n",
    "    # found all the proteins/peptides with missing values rate below\n",
    "    # the threshold, pep_columns contains the remaining protein/peptide\n",
    "    # in a pandas dataframe with $names as its column name\n",
    "    for each_column in data_object[matrix_name].loc[\n",
    "            :, ~data_object[matrix_name].columns.str.contains(\n",
    "                name)].columns:\n",
    "        # replace \"nan\" to np.nan\n",
    "        protein_columns = protein_columns.replace({\"nan\": np.nan}) \n",
    "        #find missing values and increment those rows (a row is a protein/peptide) total number of missing values\n",
    "\n",
    "        protein_columns.loc[(protein_columns[each_column] != \"MS2\")\n",
    "                            &(protein_columns[each_column] != \"MBR\")\n",
    "                            &(protein_columns[each_column] != \"ID\"), #this is more robust than using nan's in case something fails to convert\n",
    "                             \"missingValues\"] += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    protein_columns = protein_columns.assign(missingValuesRate=(\n",
    "        protein_columns[\"missingValues\"] / i) * 100)\n",
    "    \n",
    "    protein_columns = protein_columns.query(\n",
    "        \"missingValuesRate < @missing_value_thresh\")\n",
    "    \n",
    "    protein_columns = protein_columns.loc[:,\n",
    "                                  protein_columns.columns.str.contains(name)]\n",
    "\n",
    "    # filter the data_object with the remaining proteins/peptides names\n",
    "    data_object[abundance_name] = protein_columns.merge(\n",
    "        data_object[abundance_name])\n",
    "    data_object[matrix_name] = protein_columns.merge(\n",
    "        data_object[matrix_name])\n",
    "    data_object[other_info_name] = protein_columns.merge(\n",
    "        data_object[other_info_name])\n",
    "    # In case there is mismatch between ID table and abundance table,\n",
    "    # mannually remove the row with all NaN values\n",
    "    # keep rows in data_object[abundance_name] where at least two values are \n",
    "    # not NaN(do this to all rows except the first row), otherwise can't\n",
    "    # calculate the stdev\n",
    "    if ignore_nan:\n",
    "        data_object[abundance_name] = data_object[abundance_name].dropna(\n",
    "            thresh=2, subset=data_object[abundance_name].columns[1:])\n",
    "        # This will cause the veen diagram to be different from R program\n",
    "    \n",
    "    return data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_missing_values_MS2(data_object,\n",
    "                             missing_value_thresh=33,\n",
    "                             is_protein=True,\n",
    "                             ignore_nan=False):\n",
    "    \"\"\"_Filter out proteins/peptides with missing values rate above the\n",
    "    threshold_\n",
    "\n",
    "    Args:\n",
    "        data_object (_panada_): _dataframe contain data for one experimental\n",
    "        condition_\n",
    "        missing_value_thresh (int, optional): _description_. Defaults to 33.\n",
    "        analysis_program (str, optional): _description_.\n",
    "        ignore_nan: if filter intensity again with Nan threadshold, this \n",
    "        helps with the calcualting stdev step.\n",
    "\n",
    "    Returns:\n",
    "        _data_object_: _dictionary containing data for one experimental\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "0     A0A096LP49                            0.00\n",
    "1     A0A0B4J2D5                        89850.26\n",
    "2         A0AVT1                        83055.87\n",
    "    \"\"\"\n",
    "    if is_protein:\n",
    "        name = \"Accession\"\n",
    "        matrix_name = \"protein_ID_matrix\"\n",
    "        other_info_name = \"protein_other_info\"\n",
    "        abundance_name = \"protein_abundance\"\n",
    "        \n",
    "    else:\n",
    "        name = \"Annotated Sequence\"\n",
    "        matrix_name = \"peptide_ID_matrix\"\n",
    "        other_info_name = \"peptide_other_info\"\n",
    "        abundance_name = \"peptide_abundance\"\n",
    "\n",
    "    protein_columns = data_object[matrix_name].assign(missingValues=0)\n",
    "\n",
    "    i = 0\n",
    "    # found all the proteins/peptides with missing values rate below\n",
    "    # the threshold, pep_columns contains the remaining protein/peptide\n",
    "    # in a pandas dataframe with $names as its column name\n",
    "    for each_column in data_object[matrix_name].loc[:, ~data_object[matrix_name].columns.str.contains(name)].columns:\n",
    "        # replace \"nan\" to np.nan\n",
    "        protein_columns = protein_columns.replace({\"nan\": np.nan}) \n",
    "        protein_columns.loc[protein_columns[each_column] != \"MS2\", #ID/MBR are still missing values if you are only considering MS2\n",
    "                             \"missingValues\"] += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    protein_columns = protein_columns.assign(missingValuesRate=(\n",
    "        protein_columns[\"missingValues\"] / i) * 100)\n",
    "    \n",
    "    protein_columns = protein_columns.query(\n",
    "        \"missingValuesRate < @missing_value_thresh\")\n",
    "    \n",
    "    protein_columns = protein_columns.loc[:,\n",
    "                                  protein_columns.columns.str.contains(name)]\n",
    "\n",
    "    # filter the data_object with the remaining proteins/peptides names\n",
    "    data_object[abundance_name] = protein_columns.merge(\n",
    "        data_object[abundance_name])\n",
    "    data_object[matrix_name] = protein_columns.merge(\n",
    "        data_object[matrix_name])\n",
    "    data_object[other_info_name] = protein_columns.merge(\n",
    "        data_object[other_info_name])\n",
    "    # In case there is mismatch between ID table and abundance table,\n",
    "    # mannually remove the row with all NaN values\n",
    "    # keep rows in data_object[abundance_name] where at least two values are \n",
    "    # not NaN(do this to all rows except the first row), otherwise can't\n",
    "    # calculate the stdev\n",
    "    if ignore_nan:\n",
    "        data_object[abundance_name] = data_object[abundance_name].dropna(\n",
    "            thresh=2, subset=data_object[abundance_name].columns[1:])\n",
    "        # This will cause the veen diagram to be different from R program\n",
    "    \n",
    "    return data_object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a447500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeToMedian(abundance_data, apply_log2=False):\n",
    "    \"\"\"_Normalizes each column by multiplying each value in that column with\n",
    "    the median of all values in abundances (all experiments) and then dividing\n",
    "    by the median of that column (experiment)._\n",
    "    we find applying log2 transform first gives more robust results for PCA etc.\n",
    "    See https://pubs.acs.org/doi/10.1021/acsomega.0c02564\n",
    "    Args:\n",
    "        abundance_data (_pd_): _description_\n",
    "        apply_log2 (_bool_,): _apply log2 to all result_.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        format:\n",
    "         'abundances':        Accession  3_TrypsinLysConly_3A4_channel2\n",
    "         A0A096LP49                    0.000000e+00\n",
    "    \"\"\"\n",
    "    # all the columns/sample list\n",
    "    columns = [col for col in abundance_data.select_dtypes(include=[\n",
    "            np.number])]\n",
    "    data_matrix = abundance_data[columns].values\n",
    "    # replace 0 with nan\n",
    "    data_matrix[data_matrix == 0] = np.nan\n",
    "    medianOfAll = np.nanmedian(data_matrix)\n",
    "    \n",
    "    #normalize all median, all median/current run all protein median\n",
    "    # apply log2 to all the values if apply_log2 is True\n",
    "    if apply_log2:    \n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                np.log2(medianOfAll) * np.log2(abundance_data[each_column]) /\n",
    "                np.log2(np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan))))\n",
    "    else:\n",
    "        for each_column in columns:\n",
    "            abundance_data[each_column] = (\n",
    "                medianOfAll * abundance_data[each_column] /\n",
    "                np.nanmedian(abundance_data[\n",
    "                    each_column].replace(0, np.nan)))\n",
    "    #TODO divide by zero error encountered in log2, temporarily set to 0\n",
    "    abundance_data = abundance_data.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cvs(abundance_data):\n",
    "    \"\"\"_Calculate mean, stdev, cv for withn each protein/peptide abundance_\n",
    "\n",
    "    Args:\n",
    "        data_object (_type_): _full data frame_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _df with Accession mean, stdev, cv for each protein/peptide_\n",
    "    \"\"\"\n",
    "    if 'Accession' in abundance_data.columns:\n",
    "        name = \"Accession\"\n",
    "    if 'Annotated Sequence' in abundance_data.columns:\n",
    "        name = \"Annotated Sequence\"\n",
    "    abundance_data = abundance_data.assign(\n",
    "        intensity=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].mean(axis=1, skipna=True),\n",
    "        stdev=abundance_data.loc[:, ~abundance_data.columns.str.contains(\n",
    "            name)].std(axis=1, skipna=True),\n",
    "        CV=abundance_data.loc[:, ~abundance_data.columns.str.contains(name)].std(\n",
    "            axis=1, skipna=True) / abundance_data.loc[\n",
    "            :, ~abundance_data.columns.str.contains(name)].mean(\n",
    "            axis=1, skipna=True) * 100)\n",
    "\n",
    "    abundance_data = abundance_data.loc[:, [\n",
    "            name, \"intensity\", \"stdev\", \"CV\"]]\n",
    "    \n",
    "    return abundance_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbd1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_from_summary_stats(m1, m2, n1, n2, s1, s2, equal_var=False):\n",
    "    \"\"\"_Calculate T-test from summary using ttest_ind_from_stats from\n",
    "    scipy.stats package_\n",
    "\n",
    "    Args:\n",
    "        m1 (_type_): _mean list of sample 1_\n",
    "        m2 (_type_): mean list of sample 2_\n",
    "        n1 (_type_): sample size list of sample 1_\n",
    "        n2 (_type_): sample size list of sample 2_\n",
    "        s1 (_type_): standard deviation list of sample 1_\n",
    "        s2 (_type_): standard deviation list of sample 2_\n",
    "        equal_var (_type_, optional): False would perform Welch's\n",
    "        t-test, while set it to True would perform Student's t-test. Defaults\n",
    "        to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _list of P values_\n",
    "    \"\"\"\n",
    "\n",
    "    p_values = []\n",
    "    for i in range(len(m1)):\n",
    "        _, benjamini = ttest_ind_from_stats(\n",
    "            m1[i], s1[i], n1[i], m2[i], s2[i], n2[i], equal_var=equal_var)\n",
    "        p_values.append(benjamini)\n",
    "\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(abundance_data, k=5):\n",
    "    \"\"\"_inpute missing value from neighbor values_\n",
    "\n",
    "    Args:\n",
    "        abundance_data (_type_): _description_\n",
    "        k (int, optional): _number of neighbors used_. Defaults to 5.\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "        TODO: this knn imputer produces slightly different results (about 4%)\n",
    "        from the one in R. Need to figure out why\n",
    "    \"\"\"\n",
    "    name = abundance_data.columns[0]\n",
    "\n",
    "    names = abundance_data[name]\n",
    "    # x = abundance_data.select_dtypes(include=['float64', 'int64'])\n",
    "    # imputer = KNNImputer(n_neighbors=k)\n",
    "    # x_imputed = pd.DataFrame(imputer.fit_transform(x), columns=x.columns)\n",
    "\n",
    "\n",
    "    x = abundance_data.select_dtypes(include=['float', 'int'])\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    x_imputed = imputer.fit_transform(x)\n",
    "    x_imputed = pd.DataFrame(x_imputed, columns=x.columns)\n",
    "\n",
    "\n",
    "\n",
    "    abundance_data.loc[:, x.columns] = x_imputed.values\n",
    "    abundance_data[name] = names\n",
    "    return abundance_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a331ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculatePCA(abundance_object, infotib,log2T = False):\n",
    "    \"\"\"_inpute PCA transformed and variance explained by each principal\n",
    "    component_\n",
    "    \"\"\"\n",
    "    name = abundance_object.columns[0]\n",
    "    x = abundance_object\n",
    "    \n",
    "    sampleNames = x.columns[~x.columns.str.contains(\n",
    "        name)].to_frame(index=False)\n",
    "\n",
    "    if log2T: #apply log2 transformation\n",
    "        x = np.log2(x.loc[:, ~x.columns.str.contains(name)].T.values)\n",
    "    else:\n",
    "        x = x.loc[:, ~x.columns.str.contains(name)].T.values\n",
    "    # filter out columns with all zeros\n",
    "    is_finite_col = np.isfinite(np.sum(x, axis=0))\n",
    "    x_filtered = x[:, is_finite_col]\n",
    "\n",
    "    \n",
    "    # Instantiate PCA    \n",
    "    pca = PCA()\n",
    "    #\n",
    "    # Determine transformed features\n",
    "    #\n",
    "    x_pca = pca.fit_transform(x_filtered)\n",
    "    #\n",
    "    # Determine explained variance using explained_variance_ration_ attribute\n",
    "    #\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "    #\n",
    "    # Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "    # for visualizing the variance explained by each principal component.\n",
    "    #\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "    #\n",
    "    # convert numpy array to pandas dataframe for plotting\n",
    "    \n",
    "    pca_panda = pd.DataFrame(x_pca, columns=[\n",
    "        'PC' + str(i+1) for i in range(x_pca.shape[1])])\n",
    "    # add sample names to the dataframe\n",
    "    pca_panda = pd.concat(\n",
    "        [infotib, pca_panda], axis=1, join='inner')\n",
    "    \n",
    "    return pca_panda, exp_var_pca\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_name(data_dict, runname_list):\n",
    "    \"\"\"_Filter the data_dict based on runname_list, only keep the columns\n",
    "    of the data_dict that are in the runname_list_\n",
    "    Args:\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    # make dict for each runname, no accession/sequence\n",
    "    nameDict = dict(zip(data_dict[\"run_metadata\"][\"Run Names\"],data_dict[\"run_metadata\"][\"Run Identifier\"]))\n",
    "    \n",
    "    identifier_list = []\n",
    "    \n",
    "    identifier_list_plus = []\n",
    "    if \"Annotated Sequence\" in runname_list:\n",
    "        runname_list.remove(\"Annotated Sequence\")\n",
    "    if \"Accession\" in runname_list:\n",
    "        runname_list.remove(\"Accession\")\n",
    "    for eachName in runname_list:\n",
    "        identifier_list.append(nameDict[eachName])\n",
    "\n",
    "    for eachName in runname_list:\n",
    "        identifier_list_plus.append(nameDict[eachName])\n",
    "\n",
    "\n",
    "    filtered_data = {}\n",
    "   # filtered_data[\"meta\"] = data_dict[\"meta\"]\n",
    "    runname_list.extend([\"Annotated Sequence\",\"Accession\"])\n",
    "    identifier_list_plus.extend([\"Annotated Sequence\",\"Accession\"])\n",
    "\n",
    "    #filtered_data[\"run_metadata\"] = [item for item in data_dict[\n",
    "    #   \"run_metadata\"] if item in runname_list]\n",
    "    \n",
    "    filtered_data[\"run_metadata\"] = data_dict[\"run_metadata\"][\n",
    "        data_dict[\"run_metadata\"][\"Run Names\"].isin(\n",
    "            runname_list)]  \n",
    "    filtered_data[\"protein_abundance\"] = data_dict[\"protein_abundance\"][[\n",
    "        col for col in data_dict[\"protein_abundance\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"peptide_abundance\"] = data_dict[\"peptide_abundance\"][[\n",
    "        col for col in data_dict[\"peptide_abundance\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_other_info\"] = data_dict[\"protein_other_info\"][[\n",
    "        col for col in data_dict[\"protein_other_info\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"peptide_other_info\"] = data_dict[\"peptide_other_info\"][[\n",
    "        col for col in data_dict[\"peptide_other_info\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_ID_matrix\"] = data_dict[\"protein_ID_matrix\"][[\n",
    "        col for col in data_dict[\"protein_ID_matrix\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"peptide_ID_matrix\"] = data_dict[\"peptide_ID_matrix\"][[\n",
    "        col for col in data_dict[\"peptide_ID_matrix\"].columns if any(\n",
    "            word == col for word in identifier_list_plus)]]\n",
    "    filtered_data[\"protein_ID_Summary\"] = data_dict[\"protein_ID_Summary\"][\n",
    "        data_dict[\"protein_ID_Summary\"][\"names\"].isin(\n",
    "            identifier_list)]\n",
    "    filtered_data[\"peptide_ID_Summary\"] = data_dict[\"peptide_ID_Summary\"][\n",
    "        data_dict[\"peptide_ID_Summary\"][\"names\"].isin(\n",
    "            identifier_list)]\n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein peptide identification bar\n",
    "    plot_\n",
    "\n",
    "    Args:\n",
    "        data_dict (_type_): _description_\n",
    "    \"\"\"\n",
    "    # Create an empty dictionary to store the group names and filters\n",
    "    group_names = [key for key in saved_settings.keys() if \"Order@\" not in str(key)]\n",
    "\n",
    "    # import the data and save order\n",
    "    group_dict = {}\n",
    "\n",
    "    if plot_options[\"ID mode\"] == \"MS2\" or plot_options[\"ID mode\"] == \"total\" or plot_options[\"ID mode\"] == \"stacked\":\n",
    "        x_axis_order = saved_settings[\"Order@Conditions\"]\n",
    "    elif plot_options[\"Group By X\"] == \"ID_Mode\":\n",
    "        print(\"ERROR: x axis separation of MS2/MBR not supported\")\n",
    "    else:\n",
    "        x_axis_order = saved_settings[\"Order@\"+plot_options[\"Group By X\"]]\n",
    "    if plot_options[\"ID mode\"] == \"grouped\" or plot_options[\"ID mode\"] == \"grouped_stacked\" and plot_options[\"Group By Color\"] != \"ID_Mode\":\n",
    "        color_order = saved_settings[\"Order@\"+plot_options[\"Group By Color\"]]\n",
    "        plot_options[\"color_order\"] = color_order\n",
    "\n",
    "    plot_options[\"x_axis_order\"] = x_axis_order\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            runname_sublist)  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "        #print(group_dict[eachGroup][\"run_metadata\"])\n",
    "    #display(data_object[\"protein_ID_Summary\"])\n",
    "    #display(group_dict[eachGroup][\"protein_ID_Summary\"])\n",
    "    # create ID plots\n",
    "    # allIDs table will be used to store all experiment name, ID types (\n",
    "    # protein, peptide, MS2 and MS1 based), conditions and IDs numbers\n",
    "    allIDs = pd.DataFrame(\n",
    "        columns=[\"Names\", \"ID_Type\", \"ID_Mode\", \"Conditions\", \"IDs\"])\n",
    "\n",
    "    # loop through each group and extract IDs, put them into allIDs table\n",
    "    for eachCondition in group_names:\n",
    "        # Protein ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"protein_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"protein_ID_Summary\"].at[index, item]):\n",
    "                    # if the row with the item column is not empty,\n",
    "                    # add it to allIDs table.\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"protein\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"protein_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "        # Peptide ID summary\n",
    "        for index, row in group_dict[eachCondition][\n",
    "                \"peptide_ID_Summary\"].iterrows():\n",
    "            for item in [\"MS2_IDs\",\n",
    "                         \"MBR_IDs\",\n",
    "                         \"Total_IDs\"]:\n",
    "                if not pd.isna(group_dict[eachCondition][\n",
    "                        \"peptide_ID_Summary\"].at[index, item]):\n",
    "                    allIDs = pd.concat(\n",
    "                        [allIDs,\n",
    "                         pd.DataFrame(\n",
    "                             [[group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, \"names\"],\n",
    "                              \"peptide\",\n",
    "                               item,\n",
    "                               eachCondition,\n",
    "                               group_dict[eachCondition][\n",
    "                                 \"peptide_ID_Summary\"].at[index, item]]],\n",
    "                             columns=[\"Names\",\n",
    "                                      \"ID_Type\",\n",
    "                                      \"ID_Mode\",\n",
    "                                      \"Conditions\",\n",
    "                                      \"IDs\"])],\n",
    "                        ignore_index=True)\n",
    "    # ######################allIDs format###################\n",
    "    # name\tID_Type\tID_Mode\tConditions\tIDs\n",
    "    # file1\tpeptide\tMS2_IDs\texperimetn 1\txxxxx\n",
    "    # file2\tprotein\tMBR_IDs\texperiment 2\txxxx\n",
    "    # file3\tpeptide\tTotal_IDs\texperiment 3\txxx\n",
    "    #######################################################\n",
    "    # Calcuate mean, standard deviation and number of replicates for each\n",
    "    export_ids = allIDs.copy()\n",
    "    # choose protein or peptide\n",
    "    if plot_options[\"plot_type\"] == \"1\":  # Protein ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"protein\"]\n",
    "    elif plot_options[\"plot_type\"] == \"2\":  # Peptide ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Type\"] == \"peptide\"]\n",
    "\n",
    "    # choose total, MS2 or stacked\n",
    "    if plot_options[\"ID mode\"] == \"MS2\":  # MS2 ID\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"MS2_IDs\"]\n",
    "    elif plot_options[\"ID mode\"] == \"total\":\n",
    "        # total ID combined, if not already summed (key exist), sum them\n",
    "#         if allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"].empty:\n",
    "#             grouped = allIDs.groupby('name').agg(\n",
    "#                 {'IDs': 'sum', 'ID_Type': 'first', 'Conditions': 'first'})\n",
    "#             grouped = grouped.reset_index()\n",
    "#             grouped[\"ID_Mode\"] = \"Total_IDs\"\n",
    "#             allIDs = grouped\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "    elif plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\" \\\n",
    "        or plot_options[\"Group By Stack\"] == \"ID_Mode\" and not (plot_options[\"ID mode\"] == \"total\" or plot_options[\"ID mode\"] == \"MS2\"):  # total separated\n",
    "        pass\n",
    "    elif plot_options[\"ID mode\"] == \"MS2\":\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"MS2_IDs\"]\n",
    "    else:\n",
    "        allIDs = allIDs[allIDs[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "    toPlotIDs = allIDs.groupby([\"ID_Mode\", \"Conditions\"]).agg({\n",
    "        'IDs': ['mean', 'std', 'count'], 'ID_Type': 'first', })\n",
    "\n",
    "    # rename the columns\n",
    "    toPlotIDs.columns = ['IDs', 'stdev', 'n', 'ID_Type']\n",
    "    # reset the index after grouping\n",
    "    toPlotIDs = toPlotIDs.reset_index()\n",
    "    # calculate the confidence interval based on 95%confidence interval`\n",
    "    toPlotIDs[\"confInt\"] = t.ppf(0.975, toPlotIDs['n']-1) * \\\n",
    "        toPlotIDs['stdev']/np.sqrt(toPlotIDs['n'])\n",
    "\n",
    "    #add columns for the categories specified in settings file (the one with all the filenames)\n",
    "    standard_groups = [\"filter_in\",\"filter_out\",\"records\"]\n",
    "    categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "    for eachCategory in categories:\n",
    "        toPlotIDs[eachCategory] = \"\"\n",
    "        for eachGroup in group_names:\n",
    "            toPlotIDs.loc[toPlotIDs[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "    \n",
    "    #display(toPlotIDs)\n",
    "    fig = plot_IDChart_plotly(toPlotIDs,\n",
    "                               username=username,\n",
    "                               plot_options=plot_options)\n",
    "\n",
    "    if WRITE_OUTPUT:    \n",
    "        # export the data to csv for user downloading\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        # create the directory if it does not exist\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "        \n",
    "        for eachCategory in categories:\n",
    "            export_ids[eachCategory] = \"\"\n",
    "            for eachGroup in group_names:\n",
    "                export_ids.loc[export_ids[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "        export_ids = export_ids.replace({\"Names\": dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],data_object[\"run_metadata\"][\"Run Names\"]))})\n",
    "        \n",
    "        # export the data to csv\n",
    "        export_ids.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_ID_data.csv\"), index=False)\n",
    "        \n",
    "        print(\"Downloading links...\")\n",
    "\n",
    "        # create the link for downloading the data\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_ID_data.csv\"\n",
    "\n",
    "        # add png download link\n",
    "\n",
    "        png_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_Bar_Plot.png\"\n",
    "\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_Bar_Plot.png\"), format = \"png\", validate = False, engine = \"kaleido\", scale = 10)\n",
    "\n",
    "\n",
    "    else:\n",
    "        CSV_link = None\n",
    "        png_link = None\n",
    "    return fig, CSV_link, png_link\n",
    "\n",
    "def plot_IDChart_plotly(ID_data,\n",
    "                        username=None,\n",
    "                        plot_options=None):\n",
    "    \"\"\"_Plot the ID bar plot for the given data_\n",
    "\n",
    "    Args:\n",
    "        ID_data (_type_): _description_\n",
    "        username (str, optional): _description_. Defaults to \"test\".\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    plot_div = None\n",
    "    \n",
    "\n",
    "    if plot_options[\"ID mode\"] == \"grouped\":  \n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "        #find out present categories\n",
    "        categories = plot_options[\"color_order\"]\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Bar(name = eachCategory,\n",
    "                        x=ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,\"IDs\"].tolist(),\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        text = [round(x) for x in ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,\"IDs\"].tolist()],\n",
    "                        error_y=dict(\n",
    "                            type = \"data\",\n",
    "                            array = ID_data.loc[ID_data[plot_options[\"Group By Color\"]]==eachCategory,error_bars].tolist(),\n",
    "                            visible = error_visibile\n",
    "                        )))\n",
    "            i = i + 1                    \n",
    "\n",
    "        fig = go.Figure(data = fig_data,\n",
    "                        layout=go.Layout(yaxis_title=plot_options[\"Y Title\"],\n",
    "                        xaxis_title=plot_options[\"Group By X\"],\n",
    "                        barmode=\"group\",paper_bgcolor=\"rgba(255,255,255,255)\",\n",
    "                        plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                        yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                        xaxis=dict(showline=True, linewidth=1, linecolor='black')))\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "\n",
    "    elif plot_options[\"ID mode\"] == \"stacked\":\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "        if plot_options[\"Group By Stack\"] == \"ID_Mode\":\n",
    "            layers = [\"MS2_IDs\", \"MBR_IDs\"]\n",
    "        else:\n",
    "            layers = ID_data.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        fig_data = []\n",
    "        last_layer = None\n",
    "        i = 0\n",
    "        for eachLayer in layers: \n",
    "            if last_layer == None:\n",
    "                fig_data.append(go.Bar(\n",
    "                    name = eachLayer,\n",
    "                    x = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]].tolist(),\n",
    "                    y = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist(),\n",
    "                    marker_color = plot_options[\"color\"][i],\n",
    "                    # text = [round(x) for x in ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()],\n",
    "                    # error_y= dict(\n",
    "                    #     type = \"data\",\n",
    "                    #     array = ID_data.loc[ID_data[plot_options[\"Group By Stack\"]]==eachLayer,error_bars].tolist(),\n",
    "                    #     visible = error_visibile\n",
    "                    # )\n",
    "                ))\n",
    "                bases = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"]\n",
    "                i = i + 1\n",
    "            elif eachLayer == layers[-1]:\n",
    "                fig_data.append(go.Bar(\n",
    "                    name = eachLayer,\n",
    "                    x = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]].tolist(),\n",
    "                    y = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist(),\n",
    "                    base=bases,\n",
    "                    marker_color = plot_options[\"color\"][i],\n",
    "                    opacity=0.5,\n",
    "                    # text = [round(x) for x in bases + ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()],\n",
    "                    # textfont=plot_options[\"font\"],\n",
    "                    error_y= dict(\n",
    "                        type = \"data\",\n",
    "                        array = ID_data.loc[ID_data[plot_options[\"Group By Stack\"]]==eachLayer,error_bars].tolist(),\n",
    "                        visible = error_visibile\n",
    "                    )\n",
    "                ))\n",
    "                print(bases)\n",
    "                bases = bases + ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()\n",
    "            else:\n",
    "                fig_data.append(go.Bar(\n",
    "                    name = eachLayer,\n",
    "                    x = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]].tolist(),\n",
    "                    y = ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist(),\n",
    "                    base=bases,\n",
    "                    marker_color = plot_options[\"color\"][i],\n",
    "                    opacity=0.5,\n",
    "                    # text = [round(x) for x in bases + ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()],\n",
    "                    # error_y= dict(\n",
    "                    #     type = \"data\",\n",
    "                    #     array = ID_data.loc[ID_data[plot_options[\"Group By Stack\"]]==eachLayer,error_bars].tolist(),\n",
    "                    #     visible = error_visibile\n",
    "                    # )\n",
    "                ))\n",
    "                print(bases)\n",
    "                bases = bases + ID_data.loc[(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()\n",
    "            last_layer = eachLayer\n",
    "        fig = go.Figure(\n",
    "                data = fig_data,\n",
    "                layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                barmode=\"stack\", \n",
    "                paper_bgcolor=\"rgba(255,255,255,255)\",\n",
    "                plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                font=plot_options[\"font\"],\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            ))          \n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])            \n",
    "    elif plot_options[\"ID mode\"] == \"grouped_stacked\":\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = True\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "            error_visibile = True\n",
    "        else:\n",
    "            error_bars = \"stdev\"\n",
    "            error_visibile = False\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        if plot_options[\"Group By X\"] == \"ID_Mode\" or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\":  # total separated\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] != \"Total_IDs\"]\n",
    "        else:\n",
    "            ID_data = ID_data[ID_data[\"ID_Mode\"] == \"Total_IDs\"]\n",
    "\n",
    "        #make data tidy\n",
    "        if plot_options[\"Group By Stack\"] == \"ID_Mode\":\n",
    "            layers = [\"MS2_IDs\", \"MBR_IDs\"]\n",
    "        else:\n",
    "            layers = ID_data.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        categories = plot_options[\"color_order\"]\n",
    "        \n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            last_layer = None\n",
    "            j = 0\n",
    "            for eachLayer in layers: \n",
    "                if last_layer == None:\n",
    "                    fig_data.append(go.Bar(\n",
    "                        name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                        x = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                        y = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"],\n",
    "                        offsetgroup=i,\n",
    "                        # text = [round(x) for x in ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()],\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        # error_y = dict(\n",
    "                        #     type = \"data\",\n",
    "                        #     array = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),error_bars],\n",
    "                        #     visible=True)\n",
    "                    ))\n",
    "                    bases = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"]\n",
    "                elif eachLayer == layers[-1]:\n",
    "                    fig_data.append(go.Bar(\n",
    "                        name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                        x = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                        y = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"],\n",
    "                        base=bases,\n",
    "                        offsetgroup=i,\n",
    "                        # text = [round(x) for x in ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()+bases],\n",
    "                        # textfont=plot_options[\"font\"],\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        opacity=1/2**j,\n",
    "                        error_y = dict(\n",
    "                            type = \"data\",\n",
    "                            array = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),error_bars],\n",
    "                            visible=True)\n",
    "                        ))\n",
    "                    bases = bases + ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()\n",
    "                else:\n",
    "                    fig_data.append(go.Bar(\n",
    "                        name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                        x = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                        y = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"],\n",
    "                        base=bases,\n",
    "                        offsetgroup=i,\n",
    "                        # text = [round(x) for x in ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()+bases],\n",
    "                        marker_color = plot_options[\"color\"][i],\n",
    "                        opacity=1/2**j,\n",
    "                        # error_y = dict(\n",
    "                        #     type = \"data\",\n",
    "                        #     array = ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),error_bars],\n",
    "                        #     visible=True)\n",
    "                        ))\n",
    "                    bases = bases + ID_data.loc[(ID_data[plot_options[\"Group By Color\"]]==eachCategory)&(ID_data[plot_options[\"Group By Stack\"]]==eachLayer),\"IDs\"].tolist()\n",
    "                last_layer=eachLayer\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(\n",
    "            fig_data,\n",
    "            layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                barmode=\"group\",\n",
    "                plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                font=plot_options[\"font\"],\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            )\n",
    "        )\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "    else:\n",
    "\n",
    "        # plot options\n",
    "        # error bar\n",
    "        if plot_options[\"error bar\"] == \"stdev\":\n",
    "            error_bars = \"stdev\"\n",
    "        elif plot_options[\"error bar\"] == \"ci95\":\n",
    "            error_bars = \"confInt\"\n",
    "        else:\n",
    "            error_bars = None\n",
    "\n",
    "        # mean label\n",
    "        if plot_options[\"mean label\"] == \"True\" or \\\n",
    "                plot_options[\"mean label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                int(total)), \"showarrow\": False} for x, total in zip(\n",
    "                    ID_data[\"Conditions\"], ID_data[\"IDs\"])]\n",
    "        else:\n",
    "            total_labels = []   # no mean labels\n",
    "\n",
    "        # create the plot\n",
    "        fig = px.bar(ID_data,\n",
    "                     x=\"Conditions\",\n",
    "                     y=\"IDs\",\n",
    "                     error_y=error_bars,\n",
    "                     color=\"Conditions\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],\n",
    "                     )\n",
    "        fig.update_layout(xaxis_title=plot_options[\"X Title\"],\n",
    "                          yaxis_title=plot_options[\"Y Title\"],\n",
    "                          annotations=total_labels,\n",
    "                          font=plot_options[\"font\"],\n",
    "                          plot_bgcolor =\"rgba(255, 255, 255, 255)\",\n",
    "                          paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "                          yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                          xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "                          )\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV Violin plots ###\n",
    "def CV_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating protein CV violin plots_\n",
    "    \"\"\"\n",
    "    group_names = [key for key in saved_settings.keys() if \"Order@\" not in str(key)]\n",
    "    \n",
    "    # import the data and save order\n",
    "    group_dict = {}\n",
    "\n",
    "    if plot_options[\"CV mode\"] == \"MS2\" or plot_options[\"CV mode\"] == \"total\" or plot_options[\"CV mode\"] == \"stacked\":\n",
    "        x_axis_order = saved_settings[\"Order@Conditions\"]\n",
    "    else:\n",
    "        x_axis_order = saved_settings[\"Order@\"+plot_options[\"Group By X\"]]\n",
    "    plot_options[\"x_axis_order\"] = x_axis_order\n",
    "    if plot_options[\"CV mode\"] == \"grouped\" or plot_options[\"CV mode\"] == \"grouped_stacked\" and plot_options[\"Group By Color\"] != \"ID_Mode\":\n",
    "        color_order = saved_settings[\"Order@\"+plot_options[\"Group By Color\"]]\n",
    "        plot_options[\"color_order\"] = color_order\n",
    "    # filter runs into different groups\n",
    "    i = 1\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "\n",
    "    if plot_options[\"plot_type\"] == 1:\n",
    "        matrix_name = \"protein_abundance\"\n",
    "    elif plot_options[\"plot_type\"] == 2:\n",
    "        matrix_name = \"peptide_abundance\" \n",
    "\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "    if plot_options[\"Group By X\"] == \"ID_Mode\"or plot_options[\"Group By Color\"] == \"ID_Mode\" \\\n",
    "        or plot_options[\"Group By Stack\"] == \"ID_Mode\"and not (plot_options[\"CV mode\"] == \"total\" or plot_options[\"CV mode\"] == \"MS2\"):  # total separated\n",
    "        Intensity_dict_MS2 = {}\n",
    "        for eachGroup in group_names:\n",
    "            current_condition_data = filter_by_missing_values(\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data[matrix_name])\n",
    "            current_condition_data_MS2 = filter_by_missing_values_MS2( #returns\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict_MS2[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data_MS2[matrix_name])\n",
    "    elif plot_options[\"CV mode\"] == \"MS2\":\n",
    "        Intensity_dict_MS2 = {}\n",
    "        for eachGroup in group_names:\n",
    "            current_condition_data_MS2 = filter_by_missing_values_MS2(\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data_MS2[matrix_name])\n",
    "    else:\n",
    "        for eachGroup in group_names:\n",
    "            current_condition_data = filter_by_missing_values(\n",
    "                group_dict[eachGroup])\n",
    "            Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "                current_condition_data[matrix_name])\n",
    "\n",
    "    all_cvs = pd.DataFrame()\n",
    "\n",
    "    for eachGroup in Intensity_dict:\n",
    "        current = calculate_cvs(\n",
    "            Intensity_dict[eachGroup]).assign(Conditions=eachGroup,ID_Mode=\"All IDs\")\n",
    "        all_cvs = pd.concat([all_cvs, current], ignore_index=True)\n",
    "    if plot_options[\"Group By X\"] == \"ID_Mode\"or plot_options[\"Group By Color\"] == \"ID_Mode\"or plot_options[\"Group By Stack\"] == \"ID_Mode\"and not plot_options[\"CV mode\"] == \"total\":  # total separated  # total separated\n",
    "        print(\"MS2 CVs...\")\n",
    "        for eachGroup in Intensity_dict_MS2:\n",
    "            current = calculate_cvs(\n",
    "                Intensity_dict_MS2[eachGroup]).assign(Conditions=eachGroup,ID_Mode=\"MS2 IDs\")\n",
    "            all_cvs = pd.concat([all_cvs, current], ignore_index=True)\n",
    "\n",
    "    #add columns for the categories specified in settings file (the one with all the filenames)\n",
    "    standard_groups = [\"filter_in\",\"filter_out\",\"records\"]\n",
    "    categories = [col for col in list(saved_settings[list(group_names)[0]].keys()) if col not in standard_groups]\n",
    "    for eachCategory in categories:\n",
    "        all_cvs[eachCategory] = \"\"\n",
    "        for eachGroup in group_names:\n",
    "            all_cvs.loc[all_cvs[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "\n",
    "\n",
    "    # ######################all_CVs format###################\n",
    "#      Accession     intensity          stdev          CV   Conditions\n",
    "# 0       A6NHR9  3.248547e+06  672989.819300   20.716643    DDMandDTT\n",
    "# 1       A8MTJ3  5.031539e+05  195535.383583   38.861944    DDMandDTT\n",
    "# 2       E9PAV3  5.330290e+05  161385.491163   30.277056    DDMandDT\n",
    "    #######################################################\n",
    "\n",
    "    return plot_CV_violin(allCVs=all_cvs,\n",
    "                          username=username,\n",
    "                          plot_options=plot_options)\n",
    "\n",
    "\n",
    "def plot_CV_violin(allCVs,\n",
    "                   username=None,\n",
    "                   plot_options=None,\n",
    "                   ):\n",
    "    \"\"\"_Plot the CV violin plot for the given data._\n",
    "\n",
    "    Args:\n",
    "        allCVs (_type_): _description_\n",
    "        username (_type_, optional): _description_. Defaults to None.\n",
    "        plot_options (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    plot_div = None\n",
    "    CSV_link = None\n",
    "    png_link = None\n",
    "\n",
    "    group_names = [key for key in saved_settings.keys() if \"Order@\" not in str(key)]\n",
    "\n",
    "    allCVs_summary = allCVs.groupby([\"Conditions\"]).agg(\n",
    "        {'CV': ['median', 'mean']}).reset_index()\n",
    "    allCVs_summary[\"ID_Mode\"] = \"All IDs\"\n",
    "    temp = allCVs[allCVs[\"ID_Mode\"]==\"MS2 IDs\"].groupby([\"Conditions\"]).agg(\n",
    "        {'CV': ['median', 'mean']}).reset_index()\n",
    "    temp[\"ID_Mode\"] = \"MS2 IDs\"\n",
    "    allCVs_summary = pd.concat([temp,allCVs_summary])\n",
    "    allCVs_summary.columns = [\"Conditions\", 'meds', 'CoVar',\"ID_Mode\"]\n",
    "\n",
    "    \n",
    "    standard_groups = [\"filter_in\",\"filter_out\",\"records\"]\n",
    "    categories = [col for col in list(saved_settings[group_names[0]].keys()) if col not in standard_groups]\n",
    "    for eachCategory in categories:\n",
    "        allCVs_summary[eachCategory] = \"\"\n",
    "        for eachGroup in group_names:\n",
    "            allCVs_summary.loc[allCVs_summary[\"Conditions\"]==eachGroup,eachCategory] = saved_settings[eachGroup][eachCategory]\n",
    "\n",
    "\n",
    "    if plot_options[\"CV mode\"] == \"grouped\":  \n",
    "        if plot_options[\"median label\"] == \"True\" or \\\n",
    "                plot_options[\"median label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "                allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "        else:\n",
    "            total_labels = []  \n",
    "        #find out present categories\n",
    "        categories = plot_options[\"color_order\"] \n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "        for eachCategory in categories:\n",
    "            fig_data.append(go.Violin(name = str(eachCategory),\n",
    "                        x=allCVs.loc[allCVs[plot_options[\"Group By Color\"]]==eachCategory,\n",
    "                        plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=allCVs.loc[allCVs[plot_options[\"Group By Color\"]]==eachCategory,\"CV\"].tolist(),\n",
    "                        fillcolor = plot_options[\"color\"][i],                 \n",
    "                        box=dict(visible=bool(plot_options[\"box\"])),\n",
    "                        line={\"color\":plot_options[\"color\"][i]}\n",
    "                        ))\n",
    "            i = i + 1\n",
    "        \n",
    "                        \n",
    "        fig = go.Figure(data = fig_data)\n",
    "        \n",
    "        fig.update_layout(violinmode='group',\n",
    "                        # annotations=total_labels\n",
    "                        )\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "    elif plot_options[\"CV mode\"] == \"stacked\":  \n",
    "        if plot_options[\"median label\"] == \"True\" or \\\n",
    "                plot_options[\"median label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "                allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "        else:\n",
    "            total_labels = []      \n",
    "        #find out present categories\n",
    "        layers = allCVs.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        # create the plot\n",
    "        fig_data = []\n",
    "        #display(ID_data)\n",
    "        i = 0\n",
    "\n",
    "        for eachLayer in layers:\n",
    "            fig_data.append(go.Violin(name = eachLayer,\n",
    "                        x=allCVs.loc[allCVs[plot_options[\"Group By Stack\"]]==eachLayer,plot_options[\"Group By X\"]].tolist(),\n",
    "                        y=allCVs.loc[allCVs[plot_options[\"Group By Stack\"]]==eachLayer,\"CV\"].tolist(),\n",
    "                        box=dict(visible=bool(plot_options[\"box\"])),\n",
    "                        fillcolor = plot_options[\"color\"][i],\n",
    "                        line={\"color\":plot_options[\"color\"][i]}\n",
    "                        ))\n",
    "            i = i + 1\n",
    "        fig = go.Figure(data = fig_data)\n",
    "        \n",
    "        fig.update_layout(violinmode='overlay',\n",
    "                          plot_bgcolor='white',\n",
    "                          paper_bgcolor='white',\n",
    "                          yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                          xaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                        # annotations=total_labels\n",
    "                        )  \n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "\n",
    "    elif plot_options[\"CV mode\"] == \"grouped_stacked\":\n",
    "        #make data tidy\n",
    "        if plot_options[\"median label\"] == \"True\" or \\\n",
    "                plot_options[\"median label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "                allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "        else:\n",
    "            total_labels = []  \n",
    "        layers = allCVs.groupby(plot_options[\"Group By Stack\"]).first().reset_index()[plot_options[\"Group By Stack\"]].tolist()\n",
    "        categories = plot_options[\"color_order\"] \n",
    "        fig_data = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        for eachCategory in categories:\n",
    "            for eachLayer in layers:\n",
    "                fig_data.append(go.Violin(\n",
    "                    name = str(eachLayer) + \" \" + str(eachCategory),\n",
    "                    x = allCVs.loc[(allCVs[plot_options[\"Group By Color\"]]==eachCategory)&(allCVs[plot_options[\"Group By Stack\"]]==eachLayer),plot_options[\"Group By X\"]],\n",
    "                    y = allCVs.loc[(allCVs[plot_options[\"Group By Color\"]]==eachCategory)&(allCVs[plot_options[\"Group By Stack\"]]==eachLayer),\"CV\"],\n",
    "                    box=dict(visible=bool(plot_options[\"box\"])),\n",
    "                    offsetgroup=i,\n",
    "                    fillcolor = plot_options[\"color\"][j],\n",
    "                    line={\"color\":plot_options[\"color\"][j]}\n",
    "                    ))\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "\n",
    "        fig = go.Figure(\n",
    "            fig_data,\n",
    "            layout=go.Layout(\n",
    "                yaxis_title=plot_options[\"Y Title\"],\n",
    "                xaxis_title=plot_options[\"Group By X\"],\n",
    "                violinmode=\"group\",\n",
    "                plot_bgcolor='white',\n",
    "                paper_bgcolor='white',\n",
    "                yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                xaxis=dict(showline=True, linewidth=1, linecolor='black')\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout(violinmode='overlay',\n",
    "                          plot_bgcolor='white',\n",
    "                          paper_bgcolor='white',\n",
    "                          yaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                          xaxis=dict(showline=True, linewidth=1, linecolor='black'),\n",
    "                        # annotations=total_labels\n",
    "                        )\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "    else:\n",
    "    # create the interactive plot\n",
    "        # median label\n",
    "        if plot_options[\"median label\"] == \"True\" or \\\n",
    "                plot_options[\"median label\"] == True:\n",
    "            total_labels = [{\"x\": x, \"y\": total*1.15, \"text\": str(\n",
    "                round(total,1)), \"showarrow\": False} for x, total in zip(\n",
    "                allCVs_summary[\"Conditions\"], allCVs_summary[\"meds\"])]\n",
    "        else:\n",
    "            total_labels = []   # no median labels\n",
    "        fig = px.violin(allCVs,\n",
    "                        x=\"Conditions\",\n",
    "                        y='CV',\n",
    "                        color=\"Conditions\",\n",
    "                        box=bool(plot_options[\"box\"]),\n",
    "                        hover_data=[\"Conditions\", 'CV'],\n",
    "                        color_discrete_sequence=plot_options[\"color\"],\n",
    "                        width=plot_options[\"width\"],\n",
    "                        height=plot_options[\"height\"],\n",
    "                        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            yaxis=dict(title=plot_options[\"Y Title\"],\n",
    "                    range=plot_options[\"ylimits\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            font=plot_options[\"font\"],\n",
    "            xaxis=dict(title=plot_options[\"X Title\"], showline=True, linewidth=1, linecolor='black'),\n",
    "            showlegend=True,\n",
    "            annotations=total_labels,\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='white',\n",
    "            )\n",
    "        fig.update_xaxes(categoryorder='array', categoryarray = plot_options[\"x_axis_order\"])\n",
    "\n",
    "    if WRITE_OUTPUT:        \n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_CV_Violin_Plot.png\"), format = \"png\", validate = False, engine = \"kaleido\", scale = 10)\n",
    "        \n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        allCVs.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_all_CV.csv\"), index=False)\n",
    "        allCVs_summary.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_CV_summary.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_all_CV.csv\"\n",
    "\n",
    "        # download png link\n",
    "        png_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_CV_Violin_Plot.png\"\n",
    "\n",
    "\n",
    "    return fig, CSV_link, png_link\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85d8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def venns_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating ID veens plots (up to three groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    \n",
    "    for each_key in plot_options[\"compare groups\"]:\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[\"Order@Conditions\"]:\n",
    "            group_names.append(each_key)\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    data_set = []\n",
    "    labels_set = []\n",
    "    if plot_options[\"plot_type\"] == 1:\n",
    "        matrix_name = \"protein_abundance\"\n",
    "        molecule_name = \"Accession\"\n",
    "        is_protein = True\n",
    "    elif plot_options[\"plot_type\"] == 2:\n",
    "        matrix_name = \"peptide_abundance\"\n",
    "        molecule_name = \"Annotated Sequence\"\n",
    "        is_protein = False\n",
    "    \n",
    "    for eachGroup in group_names:\n",
    "        \n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup], is_protein=is_protein)\n",
    "\n",
    "        data_set.append(\n",
    "            set(current_condition_data[matrix_name][molecule_name].unique()))\n",
    "        labels_set.append(eachGroup)\n",
    "\n",
    "    #print(data_set)\n",
    "\n",
    "    fig = venn_to_plotly(\n",
    "        data_set,\n",
    "        labels_set,\n",
    "        plot_options=plot_options,\n",
    "        username=username)\n",
    "    CSV_link = None\n",
    "    png_link = None\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        print(\"Downloading links...\")\n",
    "        # png file link\n",
    "        png_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_ID_venns_Plot.png\"\n",
    "\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_ID_venns_Plot.png\"), format = \"png\", validate = False, engine = \"kaleido\", scale = 10)\n",
    "        \n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        i = 0\n",
    "        for eachSet in data_set:\n",
    "            pd.DataFrame({\"Accession\": list(eachSet)}).to_csv(os.path.join(\n",
    "                data_dir, f\"{username}_{labels_set[i]}_Venn.csv\"), index=False)\n",
    "            i = i + 1\n",
    "    \n",
    "    return fig, png_link, CSV_link\n",
    "\n",
    "def venn_to_plotly(L_sets,\n",
    "                   L_labels=None,\n",
    "                   plot_options=None,\n",
    "                   username=None):\n",
    "    \"\"\"_Creates a venn diagramm from a list of\n",
    "    sets and returns a plotly figure_\n",
    "    \"\"\"\n",
    "    \n",
    "    # get number of sets\n",
    "    n_sets = len(L_sets)\n",
    "\n",
    "    # choose and create matplotlib venn diagramm\n",
    "    if n_sets == 2:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn2(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn2(L_sets)\n",
    "    elif n_sets == 3:\n",
    "        if L_labels and len(L_labels) == n_sets:\n",
    "            v = venn3(L_sets, L_labels)\n",
    "        else:\n",
    "            v = venn3(L_sets)\n",
    "    # supress output of venn diagramm\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Create empty lists to hold shapes and annotations\n",
    "    L_shapes = []\n",
    "    L_annotation = []\n",
    "\n",
    "    # Define color list for sets\n",
    "    L_color = plot_options[\"color\"]\n",
    "\n",
    "    # Create empty list to make hold of min and max values of set shapes\n",
    "    L_x_max = []\n",
    "    L_y_max = []\n",
    "    L_x_min = []\n",
    "    L_y_min = []\n",
    "\n",
    "    for i in range(0, n_sets):\n",
    "\n",
    "        # create circle shape for current set\n",
    "\n",
    "        shape = go.layout.Shape(\n",
    "            type=\"circle\",\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            x0=v.centers[i][0] - v.radii[i],\n",
    "            y0=v.centers[i][1] - v.radii[i],\n",
    "            x1=v.centers[i][0] + v.radii[i],\n",
    "            y1=v.centers[i][1] + v.radii[i],\n",
    "            fillcolor=L_color[i],\n",
    "            line_color=L_color[i],\n",
    "            opacity=plot_options[\"opacity\"]\n",
    "        )\n",
    "\n",
    "        L_shapes.append(shape)\n",
    "\n",
    "        # create set label for current set\n",
    "        try:\n",
    "            anno_set_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.set_labels[i].get_position()[0],\n",
    "                y=v.set_labels[i].get_position()[1],\n",
    "                text=v.set_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_set_label)\n",
    "\n",
    "            # get min and max values of current set shape\n",
    "            L_x_max.append(v.centers[i][0] + v.radii[i])\n",
    "            L_x_min.append(v.centers[i][0] - v.radii[i])\n",
    "            L_y_max.append(v.centers[i][1] + v.radii[i])\n",
    "            L_y_min.append(v.centers[i][1] - v.radii[i])\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "\n",
    "    # determine number of subsets\n",
    "    n_subsets = sum([scipy.special.binom(n_sets, i+1)\n",
    "                     for i in range(0, n_sets)])\n",
    "\n",
    "    for i in range(0, int(n_subsets)):\n",
    "        try:\n",
    "\n",
    "            # create subset label (number of common elements for current subset\n",
    "\n",
    "            anno_subset_label = go.layout.Annotation(\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                x=v.subset_labels[i].get_position()[0],\n",
    "                y=v.subset_labels[i].get_position()[1],\n",
    "                text=v.subset_labels[i].get_text(),\n",
    "                showarrow=False\n",
    "            )\n",
    "\n",
    "            L_annotation.append(anno_subset_label)\n",
    "        except Exception as err:\n",
    "            print(f\"No set labels found {err}\")\n",
    "    # define off_set for the figure range\n",
    "    off_set = 0.2\n",
    "\n",
    "    # get min and max for x and y dimension to set the figure range\n",
    "    x_max = max(L_x_max) + off_set\n",
    "    x_min = min(L_x_min) - off_set\n",
    "    y_max = max(L_y_max) + off_set\n",
    "    y_min = min(L_y_min) - off_set\n",
    "\n",
    "    # create plotly figure\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # set xaxes range and hide ticks and ticklabels\n",
    "    fig.update_xaxes(\n",
    "        range=[x_min, x_max],\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set yaxes range and hide ticks and ticklabels\n",
    "    fig.update_yaxes(\n",
    "        range=[y_min, y_max],\n",
    "        scaleanchor=\"x\",\n",
    "        scaleratio=1,\n",
    "        showticklabels=False,\n",
    "        ticklen=0\n",
    "    )\n",
    "\n",
    "    # set figure properties and add shapes and annotations\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(b=0, l=10, pad=0, r=10, t=40),\n",
    "        width=800,\n",
    "        height=400,\n",
    "        shapes=L_shapes,\n",
    "        annotations=L_annotation,\n",
    "        title=dict(text=plot_options[\"title\"], x=0.5, xanchor='center')\n",
    "    )\n",
    "    \n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b68cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Volcano plots####\n",
    "def volcano_plots(data_object,  plot_options, saved_settings, username=None):\n",
    "    \"\"\"_Prepare data for creating intensity volcano plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    for each_key in plot_options[\"compare groups\"]:\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[\"Order@Conditions\"]:\n",
    "            group_names.append(each_key)\n",
    "\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # contain list of run names list for each groups\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1\n",
    "    # create a dictionary to store the intensity data\n",
    "    Intensity_dict = {}\n",
    "\n",
    "    for eachGroup in group_names:\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        Intensity_dict[eachGroup] = NormalizeToMedian(\n",
    "            current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "        \n",
    "        \n",
    "    group1 = group_names[0]\n",
    "    group2 = group_names[1]\n",
    "    # calculate mean, standard deviation, and the number of non-null\n",
    "    # elements for each row/protein\n",
    "    group1Data = (Intensity_dict[group1]\n",
    "                  .assign(group1_Intensity=Intensity_dict[group1].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group1_stdev=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group1_num=Intensity_dict[group1].drop(\n",
    "                      columns=['Accession']).shape[1] - Intensity_dict[\n",
    "                      group1].isna().sum(axis=1))\n",
    "                  .loc[:, ['group1_Intensity',\n",
    "                           'group1_stdev',\n",
    "                           'group1_num',\n",
    "                           'Accession']])\n",
    "    \"\"\" group1Data\n",
    "            group1_Intensity  group1_stdev  group1_num   Accession\n",
    "    0        2.824766e+05  1.708060e+05          15  A0A0B4J2D5\n",
    "    1        2.650998e+06  6.259645e+05          15      A2RUR9\n",
    "    2        1.973150e+05  5.645698e+04          15      A8MTJ3\n",
    "    3        2.524020e+05  1.355699e+05          15      A8MWD9\n",
    "    \"\"\"\n",
    "    group1Prots = group1Data.loc[:, ['Accession']]\n",
    "\n",
    "    group2Data = (Intensity_dict[group2]\n",
    "                  .assign(group2_Intensity=Intensity_dict[group2].drop(\n",
    "        columns=['Accession']).mean(axis=1),\n",
    "        group2_stdev=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).std(axis=1),\n",
    "        group2_num=Intensity_dict[group2].drop(\n",
    "                      columns=['Accession']).shape[1]\n",
    "        - Intensity_dict[group2].isna().sum(axis=1))\n",
    "        .loc[:, ['group2_Intensity',\n",
    "                 'group2_stdev',\n",
    "                 'group2_num',\n",
    "                 'Accession']])\n",
    "    # find common proteins\n",
    "    commonProts = (group2Data.loc[:, ['Accession']]\n",
    "                   .merge(group1Prots, on='Accession', how='inner'))\n",
    "    # only leave common proteins\n",
    "    group2Data = (group2Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "    group1Data = (group1Data\n",
    "                  .merge(commonProts, on='Accession', how='inner'))\n",
    "\n",
    "    group2Median = group2Data['group2_Intensity'].median(\n",
    "        )\n",
    "    group1Median = group1Data['group1_Intensity'].median(\n",
    "        )\n",
    "    #this is the median for each protein across both groups, you could do a ratio, but this puts it in terms of\n",
    "    allmedian = pd.DataFrame({\"col2\":group2Data['group2_Intensity'],\"col1\":group1Data['group1_Intensity']}).median(axis=1,numeric_only=True)\n",
    "    \n",
    "    if (Intensity_dict[group1].shape[1] > 3 and\n",
    "        Intensity_dict[group2].shape[1] > 3 and\n",
    "            group2 != group1):\n",
    "        # calculate the ratio between two group median,\n",
    "        # will be used to normalize them\n",
    "        ratio1 = allmedian / group1Median\n",
    "        ratio2 = allmedian / group2Median\n",
    "\n",
    "        # merge these two set of data together, adjust groups with ratio to \n",
    "        # median of all. Calculate pOriginal, p, significant\n",
    "        # pOriginal is a numpy array or list of p-values\n",
    "        # method is the method to be used for adjusting the p-values\n",
    "        volcanoData = (group2Data\n",
    "                       .merge(group1Data, on='Accession', how='inner'))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group1_Intensity=lambda x: volcanoData[\n",
    "                           'group1_Intensity'] * ratio1))\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(group2_Intensity=lambda x: volcanoData[\n",
    "                           'group2_Intensity'] * ratio2))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(\n",
    "                           pOriginal=t_test_from_summary_stats(\n",
    "                               m1=volcanoData['group2_Intensity'],\n",
    "                               m2=volcanoData['group1_Intensity'],\n",
    "                               s1=volcanoData['group2_stdev'],\n",
    "                               s2=volcanoData['group1_stdev'],\n",
    "                               n1=volcanoData['group2_num'],\n",
    "                               n2=volcanoData['group1_num'])))\n",
    "        # filter out rows in volcanoData that have pOriginal == nan\n",
    "        # if pOriginal is nan, then the p value will be nan\n",
    "        volcanoData = volcanoData[volcanoData['pOriginal'].notna()]\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(benjamini=multipletests(volcanoData[\n",
    "                           \"pOriginal\"], method='fdr_bh')[1]))\n",
    "\n",
    "        volcanoData = (volcanoData\n",
    "                       .assign(significant=(abs(volcanoData[\n",
    "                           'group2_Intensity'] - volcanoData[\n",
    "                           'group1_Intensity']) > 1)\n",
    "                           & (volcanoData['benjamini'] < 0.05)))\n",
    "\n",
    "        # add upRegulated, downRegulated, and notRegulated columns\n",
    "        volcanoData = volcanoData.assign(upRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"] - volcanoData[\n",
    "                \"group1_Intensity\"] > 1) & (volcanoData['significant']))\n",
    "\n",
    "        volcanoData = volcanoData.assign(downRegulated=lambda x: (\n",
    "            volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"] < -1) & (volcanoData['significant']))\n",
    "        volcanoData = volcanoData.assign(notRegulated=lambda x: (abs(\n",
    "           volcanoData[\"group2_Intensity\"]-volcanoData[\n",
    "                \"group1_Intensity\"]) <= 1) & (~volcanoData['significant']))\n",
    "        fig = plot_volcano_colored(\n",
    "            volcanoData,\n",
    "            label=f\"({group2}/{group1})\",\n",
    "            plot_options=plot_options,\n",
    "            username=username,\n",
    "        )\n",
    "        CSV_link = None\n",
    "        png_link = None\n",
    "        if WRITE_OUTPUT:\n",
    "            # create the file for donwnload\n",
    "            img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "            if not os.path.exists(img_dir):\n",
    "                Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "            fig.write_image(os.path.join(\n",
    "                img_dir, f\"{username}_abundance_volcano_Plot.png\"), format = \"png\", validate = False, engine = \"kaleido\", scale = 10)\n",
    "            # create the download CSV and its link\n",
    "\n",
    "            data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "            if not os.path.exists(data_dir):\n",
    "                Path(data_dir).mkdir(parents=True)\n",
    "            volcanoData.to_csv(os.path.join(\n",
    "                data_dir, f\"{username}_up_down_regulated_volcano.csv\"),\n",
    "                index=False)\n",
    "            print(\"Downloading links...\")\n",
    "            CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "                f\"{username}_up_down_regulated_volcano.csv\"\n",
    "\n",
    "            # download png link\n",
    "            png_link = f\"/files/{url_base}/images/\" \\\n",
    "                f\"{username}_abundance_volcano_Plot.png\"\n",
    "        \n",
    "        return fig, CSV_link, png_link\n",
    "\n",
    "\n",
    "def plot_volcano_colored(allData,\n",
    "                         label,\n",
    "                         plot_options=None,\n",
    "                         username=None,):\n",
    "    \n",
    "    total_labels = []\n",
    "    left = \"group1_Intensity\"\n",
    "    right = \"group2_Intensity\"\n",
    "    downData = allData[allData['downRegulated']\n",
    "                       == True]\n",
    "    upData = allData[allData['upRegulated'] == True]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        width=plot_options[\"width\"],\n",
    "        height=plot_options[\"height\"],)\n",
    "    if allData.shape[0] != 0:\n",
    "        fig.add_scatter(x=allData[right]-allData[left],\n",
    "                        y=-np.log10(allData[\"benjamini\"]),\n",
    "                        text=allData[\"Accession\"],\n",
    "                        mode=\"markers\", marker=dict(\n",
    "                            color=plot_options[\"all color\"]))\n",
    "    if downData.shape[0] != 0:\n",
    "        fig.add_scatter(x=downData[right]-downData[left],\n",
    "                        y=-np.log10(downData[\"benjamini\"]),\n",
    "                        text=downData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"down color\"]))\n",
    "    if upData.shape[0] != 0:\n",
    "        fig.add_scatter(x=upData[right]-upData[left],\n",
    "                        y=-np.log10(upData[\"benjamini\"]),\n",
    "                        text=upData[\"Accession\"],\n",
    "                        mode=\"markers\",\n",
    "                        marker=dict(color=plot_options[\"up color\"]))\n",
    "        fig.update_traces(\n",
    "            mode=\"markers\",\n",
    "            hovertemplate=\"%{text}<br>x=: %{x}\"\n",
    "            \" <br>y=: %{y}\")\n",
    "    fig.add_hline(y=-np.log10(0.05))\n",
    "    fig.add_vline(x=-1)\n",
    "    fig.add_vline(x=1)\n",
    "    if plot_options[\"title\"] != \"\" or plot_options[\"title\"] is not None:\n",
    "        plot_title = plot_options[\"title\"] + \" \" + label\n",
    "    else:\n",
    "        plot_title = None\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig.update_layout(\n",
    "        font=plot_options[\"font\"],\n",
    "\n",
    "        showlegend=False,\n",
    "        title=plot_title,\n",
    "        xaxis=dict(title=dict(\n",
    "            text=plot_options[\"X Title\"]), range=xlimits),\n",
    "        yaxis=dict(title=dict(\n",
    "            text=plot_options[\"Y Title\"]), range=ylimits),\n",
    "        annotations=total_labels,\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###PCA plots####\n",
    "def PCA_plots(data_object, plot_options, saved_settings,username=None):\n",
    "    \"\"\"_Prepare data for creating intensity PCA plots (two groups)_\n",
    "    \"\"\"\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    for each_key in plot_options[\"compare groups\"]:\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[\"Order@Conditions\"]:\n",
    "            group_names.append(each_key)\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # this will contain list of run names list for each groups\n",
    "    #print(saved_settings)\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1       \n",
    "        #print((runname_sublist))\n",
    "\n",
    "    all_runs =[item for sublist in runname_list for item in sublist]\n",
    "\n",
    "    # combined the data after filtering\n",
    "    #  missing values and log2 transformation/normalization\n",
    "    combined_infodata = pd.DataFrame() # store run names and group names\n",
    "    combined_pcaData = pd.DataFrame() # store normalized data and protein names\n",
    "    for eachGroup in group_names:\n",
    "\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        normalized_data = NormalizeToMedian(\n",
    "             current_condition_data[\"protein_abundance\"],apply_log2=True)\n",
    "        toFileDict = dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],data_object[\"run_metadata\"][\"Run Names\"]))\n",
    "        toFileDict = generate_column_to_name_mapping(normalized_data.columns, toFileDict)\n",
    "        normalized_data.rename(columns = toFileDict,inplace=True)\n",
    "\n",
    "        combined_infodata= pd.concat([combined_infodata, pd.DataFrame({\n",
    "            \"Sample_Groups\": normalized_data\n",
    "            .drop(\n",
    "                \"Accession\", axis=1).rename(columns = toFileDict).columns,\n",
    "            \"Type\": eachGroup})])\n",
    "        \n",
    "        '''for x in range(len(list(combined_infodata[\"Sample_Groups\"]))):\n",
    "            print(list(combined_infodata[\"Sample_Groups\"])[x])\n",
    "        '''\n",
    "        if combined_pcaData.empty:\n",
    "            combined_pcaData = normalized_data\n",
    "            print(\"Empty\")\n",
    "        else:\n",
    "            combined_pcaData = pd.merge(combined_pcaData, normalized_data)\n",
    "\n",
    "    #normalize the data\n",
    "    # using ratio of current group median value divide by the all groups median \n",
    "    # to create a scaling factor magicNUm to scale the each group\n",
    "    quant_names = group_names\n",
    "    while \"Annotated Sequence\" in quant_names:\n",
    "        quant_names.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in quant_names:\n",
    "        quant_names.remove(\"Accession\")\n",
    "\n",
    "    while \"Annotated Sequence\" in all_runs:\n",
    "        all_runs.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in all_runs:\n",
    "        all_runs.remove(\"Accession\")\n",
    "\n",
    "    for n in range(len(quant_names)-2): #-2 because not Accession and Annotated Sequence \n",
    "        if \"Annotated Sequence\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Annotated Sequence\")\n",
    "        if \"Accession\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Accession\")\n",
    "        \n",
    "        magicNum =np.nanmedian(combined_pcaData[runname_list[\n",
    "            n]].dropna(how='all').to_numpy()) /\\\n",
    "                np.nanmedian(combined_pcaData[\n",
    "            all_runs].dropna(how='all').to_numpy()) \n",
    "        for col in combined_pcaData[runname_list[\n",
    "                n]].columns:\n",
    "            combined_pcaData[col] = combined_pcaData[col]/magicNum\n",
    "\n",
    "\n",
    "    #performs k-Nearest Neighbors imputation to fill in any missing values\n",
    "    combined_pcaData = impute_knn(combined_pcaData)\n",
    "    combined_infodata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "\n",
    "    # perform PCA transform\n",
    "    combined_pcaData, exp_var_pca = CalculatePCA(combined_pcaData,\n",
    "                                                     combined_infodata)\n",
    "\n",
    "    return plot_PCA_plotly(combined_pcaData,\n",
    "                           exp_var_pca,\n",
    "                           plot_options=plot_options,\n",
    "                           username=username,\n",
    "                           )\n",
    "\n",
    "\n",
    "def plot_PCA_plotly(pca_panda,\n",
    "                    exp_var_pca,\n",
    "                    plot_options=None,\n",
    "                    username=None,):\n",
    "\n",
    "    CSV_link = None\n",
    "    png_link = None\n",
    "\n",
    "    # Assuming pca_data is a pandas dataframe containing PCA results\n",
    "    # and \"Type\" is a column in the dataframe indicating the type of sample\n",
    "    if not plot_options[\"xlimits\"] or plot_options[\"xlimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"xlimits\"], list):\n",
    "        xlimits = None\n",
    "    else:\n",
    "        xlimits = plot_options[\"xlimits\"]\n",
    "\n",
    "    if not plot_options[\"ylimits\"] or plot_options[\"ylimits\"] == \"[]\" or \\\n",
    "            not isinstance(plot_options[\"ylimits\"], list):\n",
    "        ylimits = None\n",
    "    else:\n",
    "        ylimits = plot_options[\"ylimits\"]\n",
    "\n",
    "    fig = px.scatter(pca_panda,\n",
    "                     x='PC1',\n",
    "                     y='PC2',\n",
    "                     color=\"Type\",\n",
    "                     text=\"Sample_Groups\",\n",
    "                     symbol=\"Type\",\n",
    "                     color_discrete_sequence=plot_options[\"color\"],\n",
    "\n",
    "                     symbol_sequence=plot_options[\"symbol\"],\n",
    "                     size_max=30,\n",
    "                     labels={'PC1': f'PC1 ({round(exp_var_pca[0]*100,2)}%)',\n",
    "                             'PC2': f'PC2 ({round(exp_var_pca[1]*100,2)}%)',\n",
    "                             'Type': 'Sample Type'}, title='PCA Plot',\n",
    "                     width=plot_options[\"width\"],\n",
    "                     height=plot_options[\"height\"],)\n",
    "\n",
    "    fig.update_traces(\n",
    "        mode=\"markers\",\n",
    "        marker=dict(size=plot_options[\"marker_size\"],),\n",
    "        hovertemplate=\"%{text}<br>PC1: %{x} <br>PC2: %{y}\")\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "        paper_bgcolor=\"rgba(255, 255, 255, 255)\",\n",
    "        font=plot_options[\"font\"],\n",
    "        title=plot_options[\"title\"],\n",
    "        xaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=xlimits),\n",
    "        yaxis=dict(linecolor='black',\n",
    "                   showticklabels=False, mirror=True, range=ylimits),\n",
    "    )\n",
    "    if WRITE_OUTPUT:\n",
    "        # create the file for donwnload\n",
    "        img_dir = os.path.join(APPFOLDER, \"images/\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            Path(img_dir).mkdir(parents=True)\n",
    "\n",
    "        fig.write_image(os.path.join(\n",
    "            img_dir, f\"{username}_PCA_Plot.png\"), format = \"png\", validate = False, engine = \"kaleido\", scale = 10)\n",
    "        # create the download CSV and its link\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        pca_panda.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_PCA.csv\"), index=False)\n",
    "        print(\"Downloading links...\")\n",
    "        CSV_link = f\"/files/{url_base}/csv/\" \\\n",
    "            f\"{username}_PCA.csv\"\n",
    "\n",
    "        # download png link\n",
    "        png_link = f\"/files/{url_base}/images/\" \\\n",
    "            f\"{username}_PCA_Plot.png\"\n",
    "\n",
    "    return fig, CSV_link, png_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_plots(data_object, plot_options, saved_settings, username=None):\n",
    "    group_names = []\n",
    "\n",
    "    # no compare groups is provided, compare first two\n",
    "    for each_key in plot_options[\"compare groups\"]:\n",
    "        if each_key in plot_options[\"compare groups\"] and saved_settings[\"Order@Conditions\"]:\n",
    "            group_names.append(each_key)\n",
    "    # import the data\n",
    "    group_dict = {}\n",
    "\n",
    "    # filter runs into different groups\n",
    "    i = 0\n",
    "    runname_list = []  # this will contain list of run names list for each groups\n",
    "    #print(saved_settings)\n",
    "    for eachGroup in group_names:\n",
    "        runname_sublist = saved_settings[eachGroup][\"records\"]\n",
    "        group_dict[eachGroup] = filter_by_name(\n",
    "            data_object,\n",
    "            list(runname_sublist))  # prevent the list from being changed\n",
    "        runname_list.append(runname_sublist)\n",
    "        i += 1       \n",
    "        #print((runname_sublist))\n",
    "\n",
    "    all_runs =[item for sublist in runname_list for item in sublist]\n",
    "\n",
    "    # combined the data after filtering\n",
    "    #  missing values and log2 transformation/normalization\n",
    "    combined_infodata = pd.DataFrame() # store run names and group names\n",
    "    combined_heatmap_data = pd.DataFrame() # store normalized data and protein names\n",
    "    \n",
    "    for eachGroup in group_names:\n",
    "\n",
    "        current_condition_data = filter_by_missing_values(\n",
    "            group_dict[eachGroup])\n",
    "        normalized_data = NormalizeToMedian(\n",
    "             current_condition_data[\"protein_abundance\"],apply_log2=False) #apply this later\n",
    "        toFileDict = dict(zip(data_object[\"run_metadata\"][\"Run Identifier\"],\n",
    "                              [eachGroup + \"_#\" + str(i) for i in range(len(data_object[\"run_metadata\"][\"Run Identifier\"]))]))\n",
    "        toFileDict = generate_column_to_name_mapping(normalized_data.columns, toFileDict)\n",
    "        normalized_data.rename(columns = toFileDict,inplace=True)\n",
    "\n",
    "        combined_infodata= pd.concat([combined_infodata, pd.DataFrame({\n",
    "            \"Sample_Groups\": normalized_data\n",
    "            .drop(\n",
    "                \"Accession\", axis=1).rename(columns = toFileDict).columns,\n",
    "            \"Type\": eachGroup})])\n",
    "        \n",
    "        '''for x in range(len(list(combined_infodata[\"Sample_Groups\"]))):\n",
    "            print(list(combined_infodata[\"Sample_Groups\"])[x])\n",
    "        '''\n",
    "        if combined_heatmap_data.empty:\n",
    "            combined_heatmap_data = normalized_data\n",
    "            print(\"Empty\")\n",
    "        else:\n",
    "            combined_heatmap_data = pd.merge(combined_heatmap_data, normalized_data)\n",
    "\n",
    "    #normalize the data\n",
    "    # using ratio of current group median value divide by the all groups median \n",
    "    # to create a scaling factor magicNUm to scale the each group\n",
    "    quant_names = group_names\n",
    "    while \"Annotated Sequence\" in quant_names:\n",
    "        quant_names.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in quant_names:\n",
    "        quant_names.remove(\"Accession\")\n",
    "\n",
    "    while \"Annotated Sequence\" in all_runs:\n",
    "        all_runs.remove(\"Annotated Sequence\")\n",
    "    while \"Accession\" in all_runs:\n",
    "        all_runs.remove(\"Accession\")\n",
    "\n",
    "    for n in range(len(quant_names)-2): #-2 because not Accession and Annotated Sequence \n",
    "        if \"Annotated Sequence\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Annotated Sequence\")\n",
    "        if \"Accession\" in runname_list[n]:\n",
    "            runname_list[n].remove(\"Accession\")\n",
    "        \n",
    "        magicNum =np.nanmedian(combined_heatmap_data[runname_list[\n",
    "            n]].dropna(how='all').to_numpy()) /\\\n",
    "                np.nanmedian(combined_heatmap_data[\n",
    "            all_runs].dropna(how='all').to_numpy()) \n",
    "        for col in combined_heatmap_data[runname_list[\n",
    "                n]].columns:\n",
    "            combined_heatmap_data[col] = combined_heatmap_data[col]/magicNum\n",
    "\n",
    "    \n",
    "    if WRITE_OUTPUT or plot_options[\"significant_only\"]:\n",
    "        \n",
    "\n",
    "        # How to transpose :(\n",
    "        # detect differentially expressed proteins\n",
    "        # reformat dataframe\n",
    "        new_names = []\n",
    "        for x in combined_heatmap_data.columns:\n",
    "            if x.split(sep=\"_#\")[0] in group_names:\n",
    "                new_names.append(\"Intensity\" + x)\n",
    "            else:\n",
    "                new_names.append(x)\n",
    "        renamed_data = combined_heatmap_data.copy()\n",
    "        renamed_data.columns = new_names\n",
    "        # display(combined_heatmap_data)\n",
    "        long_data = pd.wide_to_long(renamed_data.reset_index(),\n",
    "                                       stubnames=\"Intensity\",i=\"Accession\",j=\"Sample\",suffix=\".*\").reset_index()\n",
    "        \n",
    "        # if plot_options[\"agg_groups\"]:\n",
    "        #     # long_data= long_data.groupby([\"Accession\", \"Group\"]).agg({\"Intensity\": \"mean\"}).reset_index()\n",
    "\n",
    "        #     # display(long_data)\n",
    "        #     index = \"Group\"\n",
    "        #     pivoted_data =  long_data.pivot(index=index,columns=\"Accession\", values = \"Intensity\").reset_index()\n",
    "        # else:\n",
    "        #     index = \"Sample\"\n",
    "        pivoted_data =  long_data.pivot(index=\"Sample\",columns=\"Accession\", values = \"Intensity\").reset_index()\n",
    "        pivoted_data[\"Group\"] = pivoted_data[\"Sample\"].str.replace(\"_#.*\",\"\",regex=True)\n",
    "        # new_cols = combined_heatmap_data.columns.drop(\"Accession\")\n",
    "        # display(pivoted_data)\n",
    "        pvalues = []\n",
    "        means = []\n",
    "        for col in pivoted_data.columns.drop([\"Group\",\"Sample\"]):\n",
    "            if (pivoted_data[col].dropna().shape[0]>=2):\n",
    "                pvalues.append(anova(*[pivoted_data.loc[pivoted_data[\"Group\"]==x,col].dropna() for x in group_names])[0])\n",
    "                means.append([np.nanmean(pivoted_data.loc[pivoted_data[\"Group\"]==x,col].dropna()) for x in group_names])\n",
    "                # print()\n",
    "            else:\n",
    "                pvalues.append(np.nan)\n",
    "                means.append([np.nan])\n",
    "\n",
    "        fold_changes = []\n",
    "        # print(means)\n",
    "        for eachprotein in means:\n",
    "            length = len(eachprotein)\n",
    "            total = np.nansum(eachprotein)\n",
    "            current = []\n",
    "            if type(eachprotein) != list:\n",
    "                print(str(eachprotein) + \"******************\")\n",
    "                current = [np.nan for x in group_names]\n",
    "            else:\n",
    "                for eachMean in eachprotein:\n",
    "                    each_fold_change = eachMean/((total-eachMean)/(length-1)) #don't include in comparison\n",
    "                    current.append(each_fold_change)\n",
    "            fold_changes.append(current)\n",
    "\n",
    "        significances = []\n",
    "        alpha = plot_options[\"alpha\"] * 100 # anova function gives p values in %\n",
    "        log_min_FC = np.log2(plot_options[\"min_fold_change\"])\n",
    "        for i in range(len(fold_changes)):\n",
    "            current = False\n",
    "            current_FCs = fold_changes[i]\n",
    "            current_p = pvalues[i]\n",
    "            if type(current_FCs) == list:\n",
    "                for each_FC in current_FCs:\n",
    "                    if current_p < alpha and abs(np.log2(each_FC)) > log_min_FC:\n",
    "                        current = True\n",
    "            significances.append(current)\n",
    "        combined_heatmap_data[\"adjusted_p_values\"] = multipletests(pvalues, method='fdr_bh')[1] #adjust \n",
    "        combined_heatmap_data[\"fold_changes\"] = fold_changes\n",
    "        combined_heatmap_data[\"significant\"] = significances\n",
    "\n",
    "\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        i = 0\n",
    "        log_min_FC = np.log2(plot_options[\"min_fold_change\"]) # anova function gives p values in %\n",
    "        for eachGroup in group_names:\n",
    "            if eachGroup not in combined_heatmap_data.columns:\n",
    "                j = 0\n",
    "                current = []\n",
    "                for x in combined_heatmap_data[\"fold_changes\"]:\n",
    "                    current.append(combined_heatmap_data[\"significant\"][j] and abs(np.log2(x[i]))>log_min_FC)\n",
    "                    j = j + 1\n",
    "                combined_heatmap_data[eachGroup] = current\n",
    "            else:\n",
    "                print(\"ERROR: don't use group names that are also accession numbers\")\n",
    "            i = i + 1\n",
    "\n",
    "        data_dir = os.path.join(APPFOLDER, \"csv/\")\n",
    "        if not os.path.exists(data_dir):\n",
    "            Path(data_dir).mkdir(parents=True)\n",
    "        combined_heatmap_data.to_csv(os.path.join(\n",
    "            data_dir, f\"{username}_Heatmap.tsv\"),sep=\"\\t\", index=False)\n",
    "        \n",
    "    if plot_options[\"significant_only\"]:\n",
    "        print(combined_heatmap_data.shape)\n",
    "        combined_heatmap_data = combined_heatmap_data[combined_heatmap_data[\"significant\"]]\n",
    "        print(combined_heatmap_data.shape)\n",
    "        combined_heatmap_data = combined_heatmap_data.drop(columns=[\"significant\",\"adjusted_p_values\",\"fold_changes\"]+group_names)\n",
    "    elif WRITE_OUTPUT:\n",
    "        combined_heatmap_data = combined_heatmap_data.drop(columns=[\"significant\",\"adjusted_p_values\",\"fold_changes\"]+group_names)\n",
    "\n",
    "    if plot_options[\"log2_transform\"]:\n",
    "        for eachCol in combined_heatmap_data.columns.drop(\"Accession\"):\n",
    "            combined_heatmap_data[eachCol] = np.log2(combined_heatmap_data[eachCol])\n",
    "\n",
    "    figure = px.imshow(combined_heatmap_data.set_index(\"Accession\").dropna(), aspect=\"auto\")\n",
    "    CSV_link = None\n",
    "    png_link = None\n",
    "    return figure, CSV_link, png_link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "####All the functions and constants imports above############################\n",
    "### All the following sections are for configuration and plotting############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d48068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_This section read in data from the data management system or from the input\n",
    "files.\n",
    "Method 1 use process queue ID, serve address, account and password to read\n",
    "from data manage process queue_\n",
    "How to use: set process_queue_id and server_address, username and password\n",
    "\"\"\"\n",
    "\n",
    "process_queue_id =7272 # example MM PD3 6983, FP 7272\n",
    "server_address = \"10.37.240.41\"\n",
    "username = \"XiaofengXie\" #user name\n",
    "password = \"\" # DO NOT LEAVE your password when uploading to github\n",
    "if password != \"\" or process_queue_id == None:\n",
    "    queue_info, processor_info = queue_info_api(process_queue_id,\n",
    "                                                    server_address,\n",
    "                                                    username,\n",
    "                                                    password)\n",
    "    data_obj = read_file(queue_info=queue_info, processor_info=processor_info)\n",
    "else:\n",
    "    queue_info, processor_info = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a44dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method put files in, one dictionary for each analysis, if there is only one file or you are looking in all files, then you don't need @'s in your SETTINGS_FILE\n",
    "filelist = [\n",
    "             {\"input1\":\"input/75um-C12/combined_protein.tsv\",\n",
    "             \"input2\":\"input/75um-C12/combined_peptide.tsv\",\n",
    "             \"input3\":\"\",\n",
    "             \"input4\":\"\",\n",
    "             \"input5\":\"\",\n",
    "             \"process_app\": \"FragPipe\"},\n",
    "             {\"input1\":\"input/50um-C12/combined_protein.tsv\",\n",
    "             \"input2\":\"input/50um-C12/combined_peptide.tsv\",\n",
    "             \"input3\":\"\",\n",
    "             \"input4\":\"\",\n",
    "             \"input5\":\"\",\n",
    "             \"process_app\": \"FragPipe\"},\n",
    "             {\"input1\":\"input/75um-C8/combined_protein.tsv\",\n",
    "             \"input2\":\"input/75um-C8/combined_peptide.tsv\",\n",
    "             \"input3\":\"\",\n",
    "             \"input4\":\"\",\n",
    "             \"input5\":\"\",\n",
    "             \"process_app\": \"FragPipe\"}\n",
    "          \n",
    "            ]\n",
    "\n",
    "            #Order of files\n",
    "                #For FragPipe\n",
    "                    #input1= Protein with MBR\n",
    "                    #input2= Peptide with MBR\n",
    "                #For DIANN\n",
    "                    #input1= pgmatrix\n",
    "                    #input2= prmatrix \n",
    "                    #input5=Follow FragPipe order and input5= filelist\n",
    "                #For PD\n",
    "                    #input1= Protein \n",
    "                    #input2= Peptide Groups\n",
    "                    #input3= blank\n",
    "                    #input4= blank\n",
    "                    #input5= inpit_files\n",
    "\n",
    "\n",
    "SETTINGS_FILE = \"OTSPE_Fig4.txt\" #tab delimited\n",
    "    #Format\n",
    "    #Required Columns\n",
    "        #Group Name\t\n",
    "        #filter_in: a string pattern contained in desired raw filenames in analysis followed by an @ + the file number (filelist index) to look in\n",
    "        #filter_out: a string pattern contained in undesired raw filenames\n",
    "    #Extra columns\n",
    "        #can be used to group by color, layer, or x position in CV and ID graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define group names and assign analysis to different groups(web app version was done through GUI so\n",
    "# steps are taken to make sure they have same output)\n",
    "x = read_files(grouped_input_files=filelist)\n",
    "data_obj = outer_join_data_objects(x)\n",
    "\n",
    "settings_table = pd.read_table(SETTINGS_FILE,sep=\"\\t\")\n",
    "saved_settings = settings_table.set_index(\"Conditions\").to_dict(orient=\"index\")\n",
    "#any run with any of the filter_out items will not be used.\n",
    "\n",
    "for eachGroup in saved_settings:\n",
    "    i = 0\n",
    "    saved_settings[eachGroup][\"records\"] = []\n",
    "    filterOutType = type(saved_settings[eachGroup][\"filter_out\"])\n",
    "    if filterOutType == str or filterOutType == int or filterOutType == float and not pd.isna(saved_settings[eachGroup][\"filter_out\"]):\n",
    "        filterOut = str.split(saved_settings[eachGroup][\"filter_out\"],sep = \",\")\n",
    "    else:\n",
    "        filterOut = [\"M@di\"]\n",
    "    if len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) > 1: #multiple files, only some have the runs for this group\n",
    "        user_list = []\n",
    "        #add all runs from all analyses to be probed\n",
    "        for each_fileID in str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[1:]:\n",
    "            for eachIdentifier in data_obj[\"run_metadata\"][\"Run Identifier\"]:    \n",
    "                currentRun = data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Identifier\"] == eachIdentifier][\"Run Names\"] \n",
    "                if currentRun.size == 1:\n",
    "                    if each_fileID == str.split(eachIdentifier,sep=\"-\")[0] and list(currentRun)[0] not in user_list:\n",
    "                        user_list.append(list(currentRun)[0])\n",
    "                else:\n",
    "                    pass\n",
    "        #filter for runs that relate to this gorup within those analyses\n",
    "        for run_name in user_list:\n",
    "            if str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")[0] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                saved_settings[eachGroup][\"records\"].append(list(data_obj[\"run_metadata\"][data_obj[\"run_metadata\"][\"Run Names\"] == run_name][\"Run Names\"])[0]) \n",
    "            else:\n",
    "                pass\n",
    "    elif len(str.split(saved_settings[eachGroup][\"filter_in\"],sep = \"@\")) == 1: #across all files or maybe there is only one\n",
    "            for run_name in data_obj[\"run_metadata\"][\"Run Names\"]:\n",
    "                if saved_settings[eachGroup][\"filter_in\"] in run_name and (not any(item in run_name for item in filterOut)):\n",
    "                    saved_settings[eachGroup][\"records\"].append(data_obj[\"run_metadata\"][\"Run Names\"][i])  \n",
    "                i = i + 1   \n",
    "\n",
    "#add the order of each column\n",
    "ignore_columns = [\"filter_in\",\"filter_out\"]\n",
    "category_columns = [x for x in settings_table.columns.to_list() if x not in ignore_columns]\n",
    "\n",
    "for eachCol in category_columns:\n",
    "    saved_settings[\"Order@\"+eachCol] = settings_table[eachCol].drop_duplicates().to_list()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID plot(protein and peptides)\n",
    "plot_options={\n",
    "            \"mean label\": \"True\",\n",
    "            \"error bar\": \"stdev\", #None, stdev, or ci95\n",
    "            \"X Title\": \"Conditions\",\n",
    "            \"Y Title\": \"Protein Identifications\",\n",
    "        \"color\": [\"#D6BA73\", \"#857E7B\",\"#59344F\", \"#E9E3B4\", \"#C6878F\", \"#fac8d3\",\"#F39B6D\",  \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"black\",\"red\",\n",
    "                  \"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"],\n",
    "            \"width\": 2400,\n",
    "            \"height\": 2400,\n",
    "            \"font\": dict(size=16, family=\"Arial black\",color=\"black\"),\n",
    "            \"ID mode\": \"grouped_stacked\",#grouped, total, MS2, grouped_stacked, stacked\n",
    "            \"Group By X\": \"Sample Amount\", #ID_Mode does MS2 vs MBR\n",
    "            \"Group By Color\": \"Trap Type\",#ID_Mode does MS2 vs MBR\n",
    "            \"Group By Stack\": \"ID_Mode\",#ID_Mode does MS2 vs MBR\n",
    "        }\n",
    "plot_options[\"plot_type\"] = \"1\" # 1 is protein, 2 is peptide\n",
    "figure ,_ ,_ =ID_plots(data_obj, plot_options, saved_settings,username=\"4B\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f860d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID plot(protein and peptides)\n",
    "plot_options={\n",
    "            \"mean label\": \"True\",\n",
    "            \"error bar\": \"stdev\", #None, stdev, or ci95\n",
    "            \"X Title\": \"Conditions\",\n",
    "            \"Y Title\": \"Identified Proteins\",\n",
    "        \"color\": [\"#D6BA73\", \"#857E7B\",\"#59344F\", \"#E9E3B4\", \"#C6878F\", \"#fac8d3\",\"#F39B6D\",  \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"black\",\"red\",\n",
    "                  \"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"],\n",
    "            \"width\": 2400,\n",
    "            \"height\": 2400,\n",
    "            \"font\": dict(size=16, family=\"Arial black\",color=\"black\"),\n",
    "            \"ID mode\": \"grouped_stacked\",#grouped, total, MS2, grouped_stacked, stacked\n",
    "            \"Group By X\": \"Sample Amount\", #ID_Mode does MS2 vs MBR\n",
    "            \"Group By Color\": \"Trap Type\",#ID_Mode does MS2 vs MBR\n",
    "            \"Group By Stack\": \"ID_Mode\",#ID_Mode does MS2 vs MBR\n",
    "        }\n",
    "plot_options[\"plot_type\"] = \"2\" # 1 is protein, 2 is peptide\n",
    "figure ,_ ,_ =ID_plots(data_obj, plot_options, saved_settings, username=\"4D\")\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac3cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV violin plot\n",
    "plot_options={    \n",
    "        \"median label\": \"True\", #only works for total and MS2, can be found in CV_summary if you set WRITE_OUTPUT = True\n",
    "        \"box\": False,\n",
    "        \"X Title\": \"Conditions (protein)\",\n",
    "        \"Y Title\": \"CV of Abundance (%)\",\n",
    "        \"color\": [\"#D6BA73\", \"#857E7B\",\"#59344F\", \"#E9E3B4\", \"#C6878F\", \"#fac8d3\",\"#F39B6D\",  \"pink\",\n",
    "                  \"orange\", \"brown\", \"pink\", \"gray\", \"olive\", \"cyan\", \"black\",\"red\",\n",
    "                  \"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"],\n",
    "        \"width\": 2400,\n",
    "        \"height\": 2400,\n",
    "        \"font\": dict(size=16, family=\"Arial black\"),\n",
    "        \"ylimits\": [-10, 400],\n",
    "        \"CV mode\": \"grouped\",#grouped, total, MS2, grouped_stacked, stacked\n",
    "        \"Group By X\": \"Sample Amount\", #ID_Mode does MS2 vs MBR\n",
    "            \"Group By Color\": \"Trap Type\",#ID_Mode does MS2 vs MBR\n",
    "            \"Group By Stack\": \"Trap Type\" #ID_Mode does MS2 vs MBR\n",
    "    }\n",
    "plot_options[\"plot_type\"] = 1 # 1 is protein, 2 is peptide\n",
    "figure, _, _ =CV_plots(data_obj, plot_options, saved_settings,username=\"fig4A\")\n",
    "figure.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a889046d7ab3fc532e7617178168e652077a93fddee421a252a8a2c494ffb595"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
